doom_scenario_path = ../scenarios/linear_track_cues.wad
doom_map = map01

# Rewards: staying alive is good, dying is bad (negative already built in)
living_reward = 1   
death_penalty = 200

# Rendering options
screen_resolution = RES_320X240
screen_format = CRCGCB
render_hud = True
render_crosshair = false
render_weapon = true
render_decals = false
render_particles = false
window_visible = false

# make episodes start after x tics (after unholstering the gun)
episode_start_time = 14

# make episodes finish after x actions (tics)
episode_timeout = 2000

# Available buttons
available_buttons = 
	{ 
		TURN_LEFT 
		TURN_RIGHT 
		MOVE_FORWARD 
	}

# Game variables that will be in the state
available_game_variables = {HEALTH}

mode = PLAYER
doom_skill = 2

# Q-learning settings
learning_rate = 0.00025
discount_factor = 0.99
epochs = 50
learning_steps_per_epoch = 2000
replay_memory_size = 10000

# NN learning settings
batch_size = 64

# Training regime
test_episodes_per_epoch = 100

# Other parameters
frame_repeat = 12
resolution = (30, 45)
episodes_to_watch = 20

# Network
dqn = InputLayer(shape=[None, 1, resolution[0], resolution[1]], input_var=s1)
dqn = Conv2DLayer(dqn, num_filters=8, filter_size=[6, 6],
                    nonlinearity=rectify, W=HeUniform("relu"),
                    b=Constant(.1), stride=3)
dqn = Conv2DLayer(dqn, num_filters=8, filter_size=[3, 3],
                    nonlinearity=rectify, W=HeUniform("relu"),
                    b=Constant(.1), stride=2)
dqn = DenseLayer(dqn, num_units=128, nonlinearity=rectify, W=HeUniform("relu"),
                    b=Constant(.1))
dqn = DenseLayer(dqn, num_units=available_actions_count, nonlinearity=None)