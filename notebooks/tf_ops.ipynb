{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Ops\n",
    "A scratchpad for working with lowlevel tf functions to determine how best to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "q_s = tf.placeholder(tf.float32, shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_sa = tf.gather_nd(q_s, a)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35143887  0.22088295  0.78431705  0.22921055]\n",
      " [ 0.21140967  0.82402759  0.79057975  0.01022777]\n",
      " [ 0.01357729  0.51656414  0.09268521  0.03675706]]\n",
      "[[0, 1], [1, 3], [2, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.22088295,  0.01022777,  0.09268521], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_s_t = np.random.rand(3, 4)\n",
    "a_t = [[0, 1], [1, 3], [2, 2]]\n",
    "print(q_s_t)\n",
    "print(a_t)\n",
    "fd = {q_s: q_s_t, a: a_t}\n",
    "sess.run(q_sa, feed_dict=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot feed value of shape (3,) for Tensor 'Placeholder_6:0', which has shape '(?, 2)'\n",
      "[[0 2]\n",
      " [1 1]\n",
      " [2 3]]\n",
      "[ 0.78431708  0.8240276   0.03675706]\n"
     ]
    }
   ],
   "source": [
    "a_t = np.asarray([2, 1, 3])\n",
    "fd = {q_s: q_s_t, a: a_t}\n",
    "try:\n",
    "    sess.run(q_sa, feed_dict=fd)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    a_t = np.column_stack([np.arange(a_t.shape[0]), a_t])\n",
    "    fd = {q_s: q_s_t, a: a_t}\n",
    "    print(a_t)\n",
    "    print(sess.run(q_sa, feed_dict=fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stable softmax (and cross-entropy)\n",
    "The following was adapted from [this website](http://python.usyiyi.cn/documents/effective-tf/12.html). The final implementation in TensorFlow for the `tf.softmax_cross_entropy_with_logits` function can be found [here](https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/compiler/tf2xla/kernels/softmax_op.cc).\n",
    "\n",
    "The softmax operator is given by:\n",
    "\n",
    "$f(\\mathbf{x}) = \\dfrac{exp(\\mathbf{x})}{\\sum_{x_i \\in \\mathbf{x}} exp(x_i)}$\n",
    "\n",
    "where $\\mathbf{x}$ is a vector composed of components ${x_i \\in \\mathbf{x}}$. Because the sum of all softmax components reduces to 1.0:\n",
    "\n",
    "$\\sum_{x_i \\in \\mathbf{x}} f(x_i) = \\sum_{x_i \\in \\mathbf{x}} \\dfrac{exp(x_i)}{\\sum_{x_i \\in \\mathbf{x}} exp(x_i)}\n",
    "= \\left ( \\dfrac{1}{\\sum_{x_i \\in \\mathbf{x}} exp(x_i)} \\right ) \\sum_{x_i \\in \\mathbf{x}} exp(x_i)\n",
    "= 1.0$\n",
    "\n",
    "we often like to interpet the softmax output as probabilities, implying that its input $\\mathbf{x}$ must represent the log probabilities, or logits:\n",
    "\n",
    "$\\dfrac{exp(x_i)}{\\sum_{x_i \\in \\mathbf{x}} exp(x_i)} = p(x_i)$\n",
    "\n",
    "$exp(x_i) = \\left ( p(x_i) \\right ) \\left ( \\sum_{x_i \\in \\mathbf{x}} exp(x_i) \\right )$\n",
    "\n",
    "$x_i = ln(\\left ( p(x_i) \\right ) \\left ( \\sum_{x_i \\in \\mathbf{x}} exp(x_i) \\right ) )\n",
    "    = ln(p(x_i)) + ln(\\sum_{x_i \\in \\mathbf{x}} exp(x_i))\n",
    "    = ln(p(x_i) + c$\n",
    "\n",
    "(Note the addition of the constant $c$ to all logits. This arises from the fact that there are infinitely many solutions to a given probablity distribution, since $f(x+k) = f(x)$).\n",
    "    \n",
    "We can easily build a softmax operator in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    exp = tf.exp(x)\n",
    "    z = tf.reduce_sum(tf.exp(x), axis=1, keep_dims=True)\n",
    "    return exp / z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.74253582e-02   1.11165622e-01   8.21409019e-01]\n",
      " [  9.97404592e-01   1.23089505e-04   2.47231880e-03]]\n",
      "[ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "x = np.array([[0.5, 1.0, 3.0], [7.0, -2.0, 1.0]])\n",
    "sess.run(tf.global_variables_initializer())\n",
    "out = sess.run(naive_softmax(x))\n",
    "print(out)\n",
    "print(np.sum(out, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the logits become too large (or small), the exponential exceeds the capacity of floating point representation, causing the softmax function to return 0.0 (if $exp(-\\infty)$ in the numerator and/or $exp(\\infty)$ in the denominator) or $\\infty$ (if $exp(\\infty)$ in the numerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.                 nan]\n",
      " [ 0.99752738  0.          0.00247262]]\n",
      "[ nan   1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.5, 1.0, 1000.0], [7.0, -1000.0, 1.0]])\n",
    "out = sess.run(naive_softmax(x))\n",
    "print(out)\n",
    "print(np.sum(out, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are usually willing to accept 0.0 in order to avoid `nan`, thus shifting the range of softmax to $[0, 1)$, which can be accomplished by simply subtracting the maximum value from each row. We can do this due to the property we found above: $f(x+k)=f(x)$, where $k=max(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    max_value = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "    x_shifted = x - max_value\n",
    "    exp = tf.exp(x_shifted)\n",
    "    z = tf.reduce_sum(exp, axis=1, keep_dims=True)\n",
    "    return exp / z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.        ]\n",
      " [ 0.99752738  0.          0.00247262]]\n",
      "[ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.5, 1.0, 1000.0], [7.0, -1000.0, 1.0]])\n",
    "out = sess.run(stable_softmax(x))\n",
    "print(out)\n",
    "print(np.sum(out, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the softmax function is now stabilized, using it in a cross-entropy loss function becomes unstable. Cross-entropy is defined as:\n",
    "\n",
    "$g(x) = \\sum_{x_i \\in \\mathbf{x}}{-p(x_i)ln(p'(x_i))}$\n",
    "\n",
    "where $p(x)$ is the true (target) probability distribution and $p'(x)$ is the predicted probability distribution. If $p'(x_i)=0$ for any $x_i \\in \\mathbf{x}$, then the cross-entropy blows up to $\\infty$ due to the $ln(0)$ term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_cross_entropy(x, y):\n",
    "    p = stable_softmax(x)\n",
    "    xent = tf.multiply(y, tf.log(p))\n",
    "    return -tf.reduce_sum(xent, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.1967341 ,  0.00259878]), array([ nan,  nan])]\n"
     ]
    }
   ],
   "source": [
    "x_stable = np.array([[0.5, 1.0, 3.0], [7.0, -2.0, 1.0]])\n",
    "x_unstable = np.array([[0.5, 1.0, 1000.0], [7.0, -1000.0, 1.0]])\n",
    "y = np.array([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0]])\n",
    "out = sess.run([naive_cross_entropy(x_stable, y), naive_cross_entropy(x_unstable, y)])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can circumvent this error by simply expanding the cross entropy expression:\n",
    "\n",
    "$g(\\mathbf{x}) = \\sum_{x_i \\in \\mathbf{x}}{-p(x_i)ln(p'(x_i))}\n",
    "= \\sum_{x_i \\in \\mathbf{x}}{-p(x_i) ln \\left(\\dfrac{e^{x_i}}{\\sum_{x_i \\in \\mathbf{x}}{e^{x_i}}}\\right)}\n",
    "= \\sum_{x_i \\in \\mathbf{x}}{-p(x_i) \\left(ln \\left(e^{x_i} \\right) - ln \\left(\\sum_{x_i \\in \\mathbf{x}}{e^{x_i}} \\right) \\right)}\n",
    "= \\sum_{x_i \\in \\mathbf{x}}{-p(x_i) \\left(x_i - ln \\left(\\sum_{x_i \\in \\mathbf{x}}{e^{x_i}} \\right) \\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because the stable softmax avoids $e^{\\infty}$, the stable cross-entropy function only fails when all $x_i \\in \\mathbf{x}=0$, a criteria that cannot be satisfied if softmax is implemented properly. Remember that $\\mathbf{x}$ in the expressions above is shifted by $max(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_cross_entropy(x, y):\n",
    "    max_value = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "    x_shifted = x - max_value\n",
    "    z = tf.reduce_sum(tf.exp(x_shifted), axis=1, keep_dims=True)\n",
    "    xent = tf.multiply(y, (x_shifted - tf.log(z)))\n",
    "    return -tf.reduce_sum(xent, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.1967341 ,  0.00259878]), array([  9.99000000e+02,   2.47568514e-03])]\n"
     ]
    }
   ],
   "source": [
    "out = sess.run([stable_cross_entropy(x_stable, y), stable_cross_entropy(x_unstable, y)])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the above implementation that $z=\\sum_{x_i \\in \\mathbf{x}}{e^{x_i}}$ is required for the stable cross-entropy implementation, a quantity lost in the softmax output. This is why the stable Tensorflow function `tf.softmax_cross_entropy_with_logits` takes logits, not softmax probabilities, as input, so that it can internally calculate $z$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vizdoom)",
   "language": "python",
   "name": "vizdoom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
