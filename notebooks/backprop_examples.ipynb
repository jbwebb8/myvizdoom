{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Examples\n",
    "While troubleshooting some exploding gradients, I decided to walk through backpropagation for the first step, line by line. Here are a couple of examples for two different loss functions: squared error and Huber loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared error is given by:\n",
    "    \n",
    "    L = np.sum((target_Q - Q)^2)\n",
    "   \n",
    "And thus the gradient with respect to Q becomes:\n",
    "\n",
    "    dL/dQ = -2 * (target_Q - Q)\n",
    "\n",
    "Because Q is given by:\n",
    "\n",
    "    Q_i = b_i + w_i,1 * x_i,1 + w_i,2 * x_i,2 + ... + w_i,n * x_i,n\n",
    "\n",
    "where `x_i` represents the input from the previous (fully-connected) layer, the gradients with respect the biases and weights of Q are:\n",
    "\n",
    "    dL/dQ_b_i = dL/dQ * 1 = dL/dQ = -2 * (target_Q - Q)\n",
    "    dL/dQ_w_i = dL/dQ * x_i = -2 * (target_Q - Q) * x_i\n",
    "\n",
    "Let's run through an example. After initializing the variables of a DQN and copying them to holder network, we can calculate q(s, a) and target_q(s, a) via our primary and target networks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q\n",
    "[[ 5.34987879 -2.93507671 -1.3103404   1.01032877]]\n",
    "\n",
    "target_Q\n",
    "[[ 4.39664125 -2.93507671 -1.3103404   1.01032877]]\n",
    "\n",
    "loss\n",
    "0.908662\n",
    "\n",
    "sum_grad\n",
    "[[ 1.  1.  1.  1.]]\n",
    "\n",
    "square_grad\n",
    "[[-1.90647507  0.          0.          0.        ]]\n",
    "\n",
    "sub_grad\n",
    "[[ 1.90647507  0.          0.          0.        ]]\n",
    "\n",
    "Q_grad\n",
    "[(array([ 1.90647507,  0.        ,  0.        ,  0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the parameters are initially equal, the target_Q will be the same for all actions, given the initial state, except that which was chosen, in this case at index 0. The update for the target_Q is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q2 = np.max(target_network.get_q_values(s2))\n",
    "target_q = target_network.get_q_values(s1)\n",
    "target_q[a] = r + gamma * (1 - isterminal) * q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By summing over the squared differences between Q and target_Q for each element, we arrive at the loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vizdoom)",
   "language": "python",
   "name": "vizdoom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
