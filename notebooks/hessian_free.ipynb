{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian-free optimization\n",
    "Attempt to implement HF optimization per []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 0: quadratic function\n",
    "First, we will compute the Hessian for the quadratic function:\n",
    "\n",
    "$f(x) = x^T A x + b^T x + c$\n",
    "\n",
    "$\\nabla f = A x + b$\n",
    "\n",
    "$H(f) = A$\n",
    "\n",
    "(assuming A is symmetric)\n",
    "\n",
    "Let's create some naive functions to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_grad(f, x):\n",
    "    return tf.gradients(f, x)\n",
    "\n",
    "def naive_hessian(f, x):\n",
    "    return tf.gradients(naive_grad(f, x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build graph\n",
    "x = tf.placeholder(tf.float32, shape=[4, 1], name='x')\n",
    "A = tf.constant(np.array([[1, 2, 3, 4], # sum = 10\n",
    "                          [2, 1, 5, 6], # sum = 14\n",
    "                          [3, 5, 1, 7], # sum = 16\n",
    "                          [4, 6, 7, 1]]), # sum = 18\n",
    "                dtype=tf.float32)\n",
    "b = tf.constant(np.array([[1], \n",
    "                          [2],\n",
    "                          [3],\n",
    "                          [4]]), \n",
    "                dtype=tf.float32)\n",
    "c = tf.constant(np.array([1]), dtype=tf.float32)\n",
    "f = 0.5 * tf.transpose(x) @ (A @ x) + tf.transpose(b) @ x + c\n",
    "#f = 0.5 * tf.matmul(tf.transpose(x), tf.matmul(A, x)) + tf.matmul(tf.transpose(b), x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x)\n",
      "[[40.]]\n",
      "\n",
      "dfdx (auto)\n",
      "[array([[11.],\n",
      "       [16.],\n",
      "       [19.],\n",
      "       [22.]], dtype=float32)]\n",
      "\n",
      "dfdx (analytic)\n",
      "[[11.]\n",
      " [16.]\n",
      " [19.]\n",
      " [22.]]\n",
      "\n",
      "df2dx2 (auto)\n",
      "[array([[10.],\n",
      "       [14.],\n",
      "       [16.],\n",
      "       [18.]], dtype=float32)]\n",
      "\n",
      "df2dx2 (analytic)\n",
      "[[1. 2. 3. 4.]\n",
      " [2. 1. 5. 6.]\n",
      " [3. 5. 1. 7.]\n",
      " [4. 6. 7. 1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x_ = np.ones([4, 1])\n",
    "feed_dict={x: x_}\n",
    "print(\"f(x)\")\n",
    "print(sess.run(f, feed_dict=feed_dict))\n",
    "print(\"\\ndfdx (auto)\")\n",
    "print(sess.run(naive_grad(f, x), feed_dict=feed_dict))\n",
    "print(\"\\ndfdx (analytic)\")\n",
    "print(sess.run(A @ x + b, feed_dict=feed_dict))\n",
    "print(\"\\ndf2dx2 (auto)\")\n",
    "print(sess.run(naive_hessian(f, x), feed_dict=feed_dict))\n",
    "print(\"\\ndf2dx2 (analytic)\")\n",
    "print(sess.run(A, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autobackprop algorithm in tensorflow returns the summation over axis 1 for dy/dx, which is exactly what occurs when A is summed over axis 1. Per the tf docs:\n",
    "\n",
    "> Constructs symbolic derivatives of sum of ys w.r.t. x in xs.\n",
    "\n",
    "> ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.\n",
    "\n",
    "> gradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.\n",
    "\n",
    "So we must slice over each component of x (that is, treat them as separate `x` in `xs`), and then stack the results back together to get the Hessian. The opposite case would occur for the Jacobian: slice over each component of `y` to get dy_i/dx, and then stack the results together.\n",
    "\n",
    "Our gradient function, however, seems okay, let's rename it for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = naive_grad\n",
    "\n",
    "def true_hessian(f, x):\n",
    "    df2dx2 = []\n",
    "    \n",
    "    # Return list of components in x\n",
    "    #x = tf.slice(x, [0, 0], [x.get_shape()[0], 1]) # not iterable\n",
    "    #x = tf.unstack(x, axis=0)\n",
    "    # This doesn't work because it creates a side branch that f does\n",
    "    # not depend on, thus returning a gradient of [None]\n",
    "    \n",
    "    # Get gradients, which returns sum(df_i/dx)\n",
    "    dfdx = grad(f, x)[0]\n",
    "    \n",
    "    # Get number of components in x\n",
    "    dfdx_dim = dfdx.get_shape().as_list()\n",
    "    dfdx_size = 1\n",
    "    for d in dfdx_dim:\n",
    "        dfdx_size *= d\n",
    "    \n",
    "    # Iterate over each component in dfdx\n",
    "    for i in range(dfdx_size):\n",
    "        begin = [i // dfdx_dim[1], i % dfdx_dim[1]]\n",
    "        #print(begin)\n",
    "        dfdx_i = tf.slice(dfdx, begin, [1, 1]) # [1, 1]\n",
    "        #print(dfdx_i)\n",
    "        df2dx_i2 = tf.gradients(dfdx_i, x)[0] # [[df/dx_idx1, df/dx_idx2, ...], [...]]\n",
    "        df2dx_i2 = tf.reshape(df2dx_i2, [1, -1]) # [df/dx_idx1, df/dx_idx2, ...]\n",
    "        df2dx2.append(df2dx_i2)\n",
    "    \n",
    "    hessian = tf.concat(df2dx2, axis=0) # [[[df/dx_1dx_1, df/dx_1dx_2, ...],\n",
    "                                        #  [df/dx_2dx_1, df/dx_2dx_2, ...],\n",
    "                                        #  ...                            ]]\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df2dx2 (corrected)\n",
      "[[1. 2. 3. 4.]\n",
      " [2. 1. 5. 6.]\n",
      " [3. 5. 1. 7.]\n",
      " [4. 6. 7. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ndf2dx2 (corrected)\")\n",
    "print(sess.run(true_hessian(f, x), feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it works! We can now rename our hessian function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hessian = true_hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Two-layer MLP\n",
    "Let's extrapolate our findings to a simple multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build graph\n",
    "x = tf.placeholder(tf.float32, shape=[3, 1], name='x')\n",
    "W1 = tf.constant(np.array([[1, 2, 3],\n",
    "                           [4, 5, 6],\n",
    "                           [7, 8, 9]]) / 10.0,\n",
    "                 dtype=tf.float32)\n",
    "b1 = tf.constant(np.ones([3, 1]), \n",
    "                 dtype=tf.float32)\n",
    "h1 = W1 @ x + b1\n",
    "z1 = tf.sigmoid(h1)\n",
    "\n",
    "W2 = tf.constant(np.array([[1, 4, 7],\n",
    "                           [2, 5, 8],\n",
    "                           [3, 6, 9]]) / 10.0,\n",
    "                 dtype=tf.float32)\n",
    "b2 = tf.constant(np.ones([3, 1]), \n",
    "                 dtype=tf.float32)\n",
    "h2 = W2 @ z1 + b2\n",
    "z2 = tf.sigmoid(h2)\n",
    "y = tf.reduce_sum(z2)\n",
    "\n",
    "# Analytic gradients\n",
    "dydz2 = tf.ones(tf.shape(z2))\n",
    "dydh2 = z2 * (1.0 - z2) * dydz2\n",
    "dydz1 = tf.transpose(W2) @ dydh2\n",
    "dydh1 = z1 * (1.0 - z1) * dydz1\n",
    "dydx = tf.transpose(W1) @ dydh1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "2.7463741\n",
      "\n",
      "dydx (auto)\n",
      "[array([[0.00771682],\n",
      "       [0.00966905],\n",
      "       [0.01162129]], dtype=float32)]\n",
      "\n",
      "dydx (analytic)\n",
      "[[0.00771682]\n",
      " [0.00966905]\n",
      " [0.01162129]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x_ = np.ones([3, 1])\n",
    "feed_dict = {x: x_}\n",
    "\n",
    "print(\"y\")\n",
    "print(sess.run(y, feed_dict=feed_dict))\n",
    "\n",
    "print(\"\\ndydx (auto)\")\n",
    "print(sess.run(grad(y, x), feed_dict=feed_dict))\n",
    "\n",
    "print(\"\\ndydx (analytic)\")\n",
    "print(sess.run(dydx, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that gradient in the auto-backpropagation returns the sum of all components of $f$ wrt each $x_i$:\n",
    "\n",
    "`tf.gradients(f, x)` --> $\n",
    "\\begin{bmatrix}\n",
    "\\sum_i \\frac{\\partial f_i}{\\partial x_1} \\\\ \n",
    "\\sum_i \\frac{\\partial f_i}{\\partial x_2} \\\\ \n",
    "... \\\\\n",
    "\\sum_i \\frac{\\partial f_i}{\\partial x_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "whereas the Jacobian of $f$ wrt $x$, $J(f, x)$, is traditionally written as:\n",
    "\n",
    "$J(f, x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_n}\\\\ \n",
    "... \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & ... & \\frac{\\partial f_m}{\\partial x_n}\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "So in order to sum over the Jacobian properly to get the equivalent of `tf.gradients`, we must transpose the Jacobian and then sum over the axis 1, which effectively sums over $f_i$ for each $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacobian(f, x):\n",
    "    # Need to iterate over f_i, analogous to iterating over x_i for hessian (?)\n",
    "    J = []\n",
    "    for f_i in tf.unstack(f, axis=0):\n",
    "        df_idx = grad(f_i, x)[0] # [[df_idx_1], [df_idx_2], ...]\n",
    "        df_idx = tf.transpose(df_idx) # [df_idx_1, df_idx_2, ...]\n",
    "        J.append(df_idx)\n",
    "        #print(df_idx.get_shape().as_list())\n",
    "    \n",
    "    J = tf.stack(J, axis=0) # [[[df_1dx_1, df_1dx_2, ...]],\n",
    "                            #  [[df_2dx_2, df_2dx_2, ...]]\n",
    "                            #                        ...]]]\n",
    "    \n",
    "    return tf.squeeze(J) # [m, 1, n] --> [m, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh2dz1 (auto)\n",
      "[[0.6]\n",
      " [1.5]\n",
      " [2.4]]\n",
      "\n",
      "dh2dz1 (analytic, Jacobian)\n",
      "[[0.1 0.4 0.7]\n",
      " [0.2 0.5 0.8]\n",
      " [0.3 0.6 0.9]]\n",
      "\n",
      "dh2dz1 (analytic, accumulated)\n",
      "[[0.6]\n",
      " [1.5]\n",
      " [2.4]]\n",
      "\n",
      "dh2dz1 (auto, Jacobian)\n",
      "[[0.1 0.4 0.7]\n",
      " [0.2 0.5 0.8]\n",
      " [0.3 0.6 0.9]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dh2dz1 (auto)\")\n",
    "print(sess.run(grad(h2, z1), feed_dict=feed_dict)[0])\n",
    "print()\n",
    "\n",
    "print(\"dh2dz1 (analytic, Jacobian)\")\n",
    "dh2dz1 = W2\n",
    "print(sess.run(dh2dz1, feed_dict=feed_dict))\n",
    "print()\n",
    "\n",
    "print(\"dh2dz1 (analytic, accumulated)\")\n",
    "print(np.sum(sess.run(dh2dz1, feed_dict=feed_dict), axis=0, keepdims=True).T)\n",
    "print()\n",
    "\n",
    "print(\"dh2dz1 (auto, Jacobian)\")\n",
    "print(sess.run(jacobian(h2, z1), feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Hessian of even a simple 3-layer MLP is not trivial to do by hand. Consider the derivative of $y$ wrt two components of the input, $x_i$ and $x_j$:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial^2 y}{\\partial x_i \\partial x_j} = H_{ij}\n",
    "&= \\frac{\\partial}{\\partial x_j} \\left (\\frac{\\partial y}{\\partial x_i} \\right ) \\\\\n",
    "&= \\frac{\\partial}{\\partial x_j} \\left ( \\sum_k \\frac{\\partial y}{\\partial h_k} \\frac{\\partial h_k}{\\partial x_i} \\right ) \\\\\n",
    "&= \\sum_k \\left [ \\frac{\\partial}{\\partial x_j} \\left ( \\frac{\\partial y}{\\partial h_k} \\frac{\\partial h_k}{\\partial x_i} \\right ) \\right ] \\\\\n",
    "&= \\sum_k \\left [ \\frac{\\partial}{\\partial x_j} \\left ( \\frac{\\partial y}{\\partial h_k} \\right ) \\frac{\\partial h_k}{\\partial x_i} + \\frac{\\partial y}{\\partial h_k} \\frac{\\partial}{\\partial x_j} \\left ( \\frac{\\partial h_k}{\\partial x_i} \\right ) \\right ] \\\\\n",
    "&= \\sum_k \\left [ \\sum_l \\left ( \\frac{\\partial^2 y}{\\partial h_k \\partial h_l} \\frac{\\partial h_l}{\\partial x_j} \\right ) \\frac{\\partial h_k}{\\partial x_i} + \\frac{\\partial y}{\\partial h_k} \\frac{\\partial^2 h_k}{\\partial x_i \\partial x_j} \\right ] \\\\\n",
    "&= \\sum_k \\sum_l \\left ( \\frac{\\partial^2 y}{\\partial h_k \\partial h_l} \\frac{\\partial h_l}{\\partial x_j} \\frac{\\partial h_k}{\\partial x_i} \\right ) + \\sum_k \\left ( \\frac{\\partial y}{\\partial h_k} \\frac{\\partial^2 h_k}{\\partial x_i \\partial x_j} \\right )\n",
    "\\end{aligned}$\n",
    "\n",
    "Clearly this scales quickly. We will manually implement the Hessian calculation per [Bishop, 1992]. Note that running the graph multiple times is not the most efficient implementation, but it does allow for the most transparency when calculating (and debugging) our manual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H (auto)\n",
      "[[-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]]\n",
      "\n",
      "H (manual)\n",
      "[[ 0.004 -0.004 -0.004 ...  0.     0.     0.   ]\n",
      " [ 0.004 -0.004 -0.004 ...  0.     0.     0.   ]\n",
      " [ 0.004 -0.004 -0.004 ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.     0.     0.    ... -0.042 -0.043 -0.041]\n",
      " [ 0.     0.     0.    ... -0.043 -0.044 -0.042]\n",
      " [ 0.     0.     0.    ... -0.044 -0.045 -0.043]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Automatic Hessian calculation\n",
    "print(\"H (auto)\")\n",
    "print(sess.run(hessian(y, W1), feed_dict=feed_dict))\n",
    "#print(sess.run(tf.hessians(y, W1), feed_dict=feed_dict))\n",
    "print()\n",
    "\n",
    "# Manual Hessian calculation\n",
    "# Get unit activations (z)\n",
    "x_ = np.ones([3, 1])\n",
    "feed_dict = {x: x_}\n",
    "x_size = x.get_shape().as_list()[0]\n",
    "h1_size = h1.get_shape().as_list()[0]\n",
    "h2_size = h2.get_shape().as_list()[0]\n",
    "num_units = x_size + h1_size + h2_size\n",
    "z_ = np.concatenate(sess.run([x, z1, z2], feed_dict=feed_dict))\n",
    "\n",
    "# Get first derivatives of activations wrt affine transformations\n",
    "dzdh_ = np.ones([num_units])\n",
    "dz2dh2 = z2 * (1.0 - z2)\n",
    "dz1dh1 = z1 * (1.0 - z1)\n",
    "dzdh_[x_size:] = np.squeeze(np.concatenate(sess.run([dz1dh1, dz2dh2], feed_dict=feed_dict)))\n",
    "#print(\"dzdh\")\n",
    "#print(dzdh_)\n",
    "#print()\n",
    "\n",
    "# Get second derivatives of activations wrt affine transformations\n",
    "d2zdh2_ = np.zeros([num_units])\n",
    "d2z2dh22 = z2 * (1 - z2) * (1 - 2*z2)\n",
    "d2z1dh12 = z1 * (1 - z1) * (1 - 2*z1)\n",
    "d2zdh2_[x_size:] = np.squeeze(np.concatenate(sess.run([d2z1dh12, d2z2dh22], feed_dict=feed_dict)))\n",
    "#print(\"d2zdh2\")\n",
    "#print(d2zdh2_)\n",
    "#print()\n",
    "\n",
    "# Create global matrix W_ that holds all parameters from all W st\n",
    "# W_[i, j] refers to the weight from unit j to unit i\n",
    "W_ = np.zeros([x_size + h1_size + h2_size, x_size + h1_size + h2_size])\n",
    "W_[x_size:x_size + h1_size, :x_size] = sess.run(W1)\n",
    "W_[x_size + h1_size:, x_size:x_size + h1_size] = sess.run(W2)\n",
    "#print(\"W_init\")\n",
    "#print(W_)\n",
    "#print()\n",
    "\n",
    "# g_li represents da_l/da_i and is found by forward propagation. Thus, the initial conditions\n",
    "# set values for da_i/da_i = 1 and begin at the first unit.\n",
    "g_ = np.zeros([num_units, num_units])\n",
    "g_[np.arange(len(g_)), np.arange(len(g_))] = 1.0\n",
    "#print(\"g_init\")\n",
    "#print(g_)\n",
    "#print()\n",
    "for i in range(x_size + h1_size + h2_size):\n",
    "    for j in range(x_size + h1_size + h2_size):\n",
    "        #for k in range(x_size + h1_size + h2_size):\n",
    "        #    g_[j, i] += dzdh_[k] * W_[j, k] * g_[k, i]\n",
    "        g_[j, i] += np.sum(dzdh_ * W_[j, :] * g_[:, i])\n",
    "\n",
    "#print(\"g\")\n",
    "#print(g_)\n",
    "#print()\n",
    "\n",
    "# o_n represents dy/da_n and is found by backpropagation. Thus, the initial conditions\n",
    "# set values for the output units m and begin at the output units.\n",
    "o_ = np.zeros([num_units])\n",
    "o_[-h2_size:] = dzdh_[-h2_size:] * 1.0\n",
    "for i in range(num_units)[::-1]:\n",
    "    for j in range(num_units)[::-1]:\n",
    "        o_[i] += dzdh_[i] * W_[j, i] * o_[j]\n",
    "\n",
    "#print(\"o\")\n",
    "#print(o_)\n",
    "#print()\n",
    "\n",
    "# b_ni represents do_n/da_i and is also found by backpropagation, via applying the \n",
    "# product rule after substituting for o_n. The initial conditions thus also set values \n",
    "# for the output units.\n",
    "b_ = np.zeros([num_units, num_units])\n",
    "b_[-h2_size:, x_size:] = d2zdh2_[-h2_size:, np.newaxis] * 1.0 * g_[-h2_size:, x_size:]\n",
    "i_range = range(num_units)\n",
    "for i in i_range[::-1]:\n",
    "    if i in i_range[:x_size]:\n",
    "        n_range = i_range[:]\n",
    "    elif i in i_range[:x_size + h1_size]:\n",
    "        n_range = i_range[x_size:]\n",
    "    else:\n",
    "        n_range = i_range[x_size + h1_size:]\n",
    "    \n",
    "    for n in n_range[::-1]:\n",
    "        for r in range(num_units)[::-1]:\n",
    "            b_[n, i] += d2zdh2_[n] * g_[n, i] * W_[r, n] * o_[r] + dzdh_[n] * W_[r, n] * b_[r, i]\n",
    "#print(\"b\")\n",
    "#print(b_)\n",
    "#print()\n",
    "\n",
    "# Finally, the Hessian is calculated using the equation:\n",
    "#     \n",
    "#    H(W_ij, W_nl) = z_j * o_n * dzdh_l * g_li + z_j * z_l * b_ni\n",
    "#    \n",
    "# where i, j, n, l are indices of the global weight matrix.\n",
    "H_ = np.zeros([W_.size, W_.size])\n",
    "for r in range(W_.size):\n",
    "    for s in range(W_.size):\n",
    "        i = r // W_.shape[1]\n",
    "        j = r % W_.shape[1]\n",
    "        n = s // W_.shape[1]\n",
    "        l = s % W_.shape[1]\n",
    "        H_[r, s] = z_[j] * o_[n] * dzdh_[l] * g_[l, i] + z_[j] * z_[l] * b_[n, i]\n",
    "\n",
    "print(\"H (manual)\")\n",
    "print(H_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our automatic hessian function (using tensorflow) shapes and slices parameters specific to the query, our manual method calculates the Hessian of a global matrix that includes interactions amongst all parameters. To find the Hessian of the parameters in `W_in`, for example, we must slice our total Hessian matrix accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_W_idx(in_idx, out_idx, num_units):\n",
    "    i, j = np.meshgrid(out_idx * num_units, in_idx, indexing='ij')\n",
    "    W_idx = (i + j).flatten()\n",
    "    return W_idx\n",
    "\n",
    "def get_W_idx(layer_sizes):\n",
    "    # Calculate total number of parameters\n",
    "    num_params = 0\n",
    "    for size in layer_sizes:\n",
    "        num_params += size\n",
    "    \n",
    "    # Slice parameters at cutoffs corresponding to unrolled weight matrices\n",
    "    idx = [] # each component represents indices of weight matrix\n",
    "    param = 0 # current param\n",
    "    params = np.arange(num_params)\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        idx1 = params[param:param + layer_sizes[i-1]] # unit indices of current layer\n",
    "        param += layer_sizes[i-1]\n",
    "        idx2 = params[param:param + layer_sizes[i]] # unit indices of next layer\n",
    "        idx.append(_get_W_idx(idx1, idx2, num_params)) # params connecting current to next layer\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]]\n"
     ]
    }
   ],
   "source": [
    "idx = get_W_idx([x_size, h1_size, h2_size]) # param indices\n",
    "idx = [np.meshgrid(idx_, idx_, indexing='ij') for idx_ in idx] # slices\n",
    "H_W1 = H_[idx[0]]\n",
    "print(H_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H_W1 (auto)\n",
      "[[-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]]\n",
      "\n",
      "H_W1 (manual)\n",
      "[[-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.004 -0.004 -0.004 -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.007 -0.007 -0.007 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.    -0.005 -0.005 -0.005]]\n",
      "\n",
      "H_W2 (auto)\n",
      "[[-0.052 -0.057 -0.06   0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.057 -0.064 -0.067  0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.06  -0.067 -0.07   0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.044 -0.049 -0.051  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.049 -0.054 -0.057  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.051 -0.057 -0.059  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.036 -0.04  -0.042]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.04  -0.045 -0.047]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.042 -0.047 -0.049]]\n",
      "\n",
      "H_W2 (manual)\n",
      "[[-0.052 -0.057 -0.06   0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.057 -0.064 -0.067  0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.06  -0.067 -0.07   0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.044 -0.049 -0.051  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.049 -0.054 -0.057  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.051 -0.057 -0.059  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.036 -0.04  -0.042]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.04  -0.045 -0.047]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.042 -0.047 -0.049]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare all matrices\n",
    "print(\"H_W1 (auto)\")\n",
    "print(sess.run(hessian(y, W1), feed_dict=feed_dict))\n",
    "print()\n",
    "\n",
    "print(\"H_W1 (manual)\")\n",
    "print(H_[idx[0]])\n",
    "print()\n",
    "\n",
    "print(\"H_W2 (auto)\")\n",
    "print(sess.run(hessian(y, W2), feed_dict=feed_dict))\n",
    "print()\n",
    "\n",
    "print(\"H_W2 (manual)\")\n",
    "print(H_[idx[1]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Static RNN\n",
    "Let's now attempt to further extend our findings to a simple recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-order gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StaticRNN:\n",
    "    def __init__(self, trace_length, x_size, h_size, y_size, seed=1234):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(1234)\n",
    "\n",
    "        # Build new graph with single W variable\n",
    "        self.x = tf.placeholder(tf.float32, shape=[trace_length, 1, x_size], name='x')\n",
    "        \n",
    "        with tf.name_scope(\"params\"):\n",
    "            # Input weights\n",
    "            self.W1 = tf.random_normal([x_size, h_size], dtype=tf.float32, name='W1')\n",
    "            self.b1 = tf.ones([1, h_size], dtype=tf.float32, name='b1')\n",
    "\n",
    "            # Recurrent weights\n",
    "            self.W2 = tf.random_normal([h_size, h_size], dtype=tf.float32, name='W2')\n",
    "            self.b2 = tf.ones([1, h_size], dtype=tf.float32, name='b2')\n",
    "\n",
    "            # Ouput weights\n",
    "            self.W3 = tf.random_normal([h_size, y_size], dtype=tf.float32, name='W3')\n",
    "            self.b3 = tf.random_normal([1, y_size], dtype=tf.float32, name='b3')\n",
    "            \n",
    "            # Vectors containing all parameters\n",
    "            self.W = tf.Variable(tf.concat([tf.reshape(self.W1, [-1]), \n",
    "                                            tf.reshape(self.W2, [-1]),\n",
    "                                            tf.reshape(self.W3, [-1])],\n",
    "                                           axis=0), name='W')\n",
    "            self.b = tf.Variable(tf.concat([tf.reshape(self.b1, [-1]),\n",
    "                                            tf.reshape(self.b2, [-1]),\n",
    "                                            tf.reshape(self.b3, [-1])],\n",
    "                                           axis=0), name='b')\n",
    "\n",
    "            # Get layer variables from concatenated vectors\n",
    "            self.W_in = tf.reshape(self.W[:x_size*h_size], [x_size, h_size], name='W_in')\n",
    "            self.b_in = tf.reshape(self.b[:x_size], [1, x_size], name='b_in')\n",
    "            self.W_rec = tf.reshape(self.W[x_size*h_size:x_size*h_size + h_size**2], \n",
    "                                    [h_size, h_size], name='W_rec')\n",
    "            self.b_rec = tf.reshape(self.b[x_size:x_size + h_size], [1, h_size], name='b_rec')\n",
    "            self.W_out = tf.reshape(self.W[-h_size*y_size:], [h_size, y_size], name='W_out')\n",
    "            self.b_out = tf.reshape(self.b[-y_size:], [1, y_size], name='b_out')\n",
    "\n",
    "        # Initialize time series [x_t, h_t, y_t]\n",
    "        xs = tf.unstack(self.x, axis=0, name='xs')\n",
    "        self.h_init = tf.zeros([1, h_size], name='h_init')\n",
    "        z = []\n",
    "        h = [self.h_init]\n",
    "        y = []\n",
    "        \n",
    "        # Create unrolled hidden layer for each time step\n",
    "        for i, x_t in enumerate(xs):\n",
    "            with tf.name_scope(\"hidden_%d\" % (i+1)):\n",
    "                z_tx = x_t @ self.W_in\n",
    "                z_th = h[-1] @ self.W_rec\n",
    "                z_t = tf.add_n([z_tx, z_th, self.b_rec], name='z_%d' % (i+1))\n",
    "                h_t = tf.tanh(z_t, name='h_%d' % (i+1))\n",
    "            \n",
    "            with tf.name_scope(\"y_%d\" % (i+1)):\n",
    "                y_t = tf.add(h_t @ self.W_out, self.b_out, name='y_%d' % (i+1))\n",
    "            \n",
    "            z.append(z_t)\n",
    "            h.append(h_t)\n",
    "            y.append(y_t)\n",
    "        \n",
    "        # Save lists of time series values\n",
    "        self.z = z\n",
    "        self.h = h[1:]\n",
    "        self.y = y\n",
    "        self.J = tf.reduce_sum(self.y, name='J')\n",
    "\n",
    "        # Save dimensions\n",
    "        self.trace_length = trace_length # unrolled time dimension\n",
    "        self.x_size = x_size\n",
    "        self.h_size = h_size\n",
    "        self.y_size = y_size\n",
    "    \n",
    "    @property\n",
    "    def auto_grads(self):\n",
    "        y = self.J\n",
    "        xs = [self.W_in, self.W_rec, self.W_out]\n",
    "        return [tf.gradients(y, x) for x in xs]\n",
    "    \n",
    "    @property\n",
    "    def manual_grads(self):\n",
    "        dJdW_in = [] # dJdW_in = Σ(dJ/dy * dy/dh * dh/dz * dz/dW_in) for [0, t]\n",
    "        dJdW_rec = [] # dJdW_rec = Σ(dJ/dy * dy/dh * dh/dz * dz/dW_rec) for [0, t]\n",
    "        dJdW_out = [] # dJdW_out = Σ(dJ/dy * dy/dW_out) for [0, t]\n",
    "\n",
    "        for t in range(self.trace_length)[::-1]: # Σ(dJ/dq * ... * dq/dW) for t' in [0, t]\n",
    "            # Chain rule: dJ_t/dz_j * dz_j/dW_in = dJ_t/dz_t * Σ(dz_t/dz_j) * dz_j/dW_in\n",
    "            dJ_tdW_in = []\n",
    "            for j in range(t+1)[::-1]: # Σ(dz_t/dz_j) for j in [0, t]\n",
    "                # Compute dJ_t/dz_t\n",
    "                dJ_tdy_t = tf.ones([y_size, 1])\n",
    "                dy_tdh_t = self._dydh()\n",
    "                dh_tdz_j = self._dhdz(self.h[t])\n",
    "                \n",
    "                # Compute dz_t/dz_j = Π(dz_k/dz_(k-1)) for k in [j, t]\n",
    "                for k in range(j, t)[::-1]:\n",
    "                    dh_tdz_j = (self._dhdz(self.h[k]) @ self._dzdh()) @ dh_tdz_j\n",
    "                \n",
    "                # Compute dz_j/dW_in\n",
    "                dz_jdW_in = self._dzdW_in(j)\n",
    "                \n",
    "                # Add dJ_t/dz_j * dz_j/dW_in\n",
    "                dJ_tdW_in.append((dz_jdW_in @ (dh_tdz_j @ (dy_tdh_t @ dJ_tdy_t))))\n",
    "            \n",
    "            # Chain rule: dJ_t/dz_j * dz_j/dW_rec = dJ_t/dz_t * Σ(dz_t/dz_j) * dz_j/dW_rec\n",
    "            dJ_tdW_rec = []\n",
    "            for j in range(t+1)[::-1]: # Σ(dz_t/dz_j) for j in [0, t-1]\n",
    "                # Compute dJ_t/dz_t\n",
    "                dJ_tdy_t = tf.ones([y_size, 1])\n",
    "                dy_tdh_t = self._dydh()\n",
    "                dh_tdz_j = self._dhdz(self.h[t])\n",
    "                \n",
    "                # Compute dz_t/dz_j = Π(dz_k/dz_(k-1)) for k in [j, t-1]\n",
    "                for k in range(j, t)[::-1]:\n",
    "                    dh_tdz_j = (self._dhdz(self.h[k]) @ self._dzdh()) @ dh_tdz_j\n",
    "                \n",
    "                # Compute dz_j/dW_rec\n",
    "                dz_jdW_rec = self._dzdW_rec(j)\n",
    "                \n",
    "                # Add dJ_t/dz_j * dz_j/dW_rec\n",
    "                dJ_tdW_rec.append((dz_jdW_rec @ (dh_tdz_j @ (dy_tdh_t @ dJ_tdy_t))))\n",
    "            \n",
    "            # dJ_tdW_out = dJ/dy * dy/dW_out\n",
    "            dJ_tdy_t = tf.ones([y_size, 1])\n",
    "            dy_tdW_out = self._dydW_out(t)\n",
    "            dJ_tdW_out = (dy_tdW_out @ dJ_tdy_t)\n",
    "            \n",
    "            # Append to lists of dJ_tdW for all t\n",
    "            dJdW_in.append(tf.add_n(dJ_tdW_in)) # sum over j\n",
    "            dJdW_rec.append(tf.add_n(dJ_tdW_rec)) # sum over j\n",
    "            dJdW_out.append(dJ_tdW_out)\n",
    "        \n",
    "        # Sum over all t\n",
    "        dJdW_in = tf.add_n(dJdW_in)\n",
    "        dJdW_rec = tf.add_n(dJdW_rec)\n",
    "        dJdW_out = tf.add_n(dJdW_out)\n",
    "        \n",
    "        # Reshape from vectorized to matrix form \n",
    "        # Transpose needed due to reshaping order (rows, then columns)\n",
    "        dJdW_in = tf.reshape(dJdW_in, [self.h_size, self.x_size]) # reverse order\n",
    "        dJdW_in = tf.transpose(dJdW_in)\n",
    "        dJdW_rec = tf.reshape(dJdW_rec, [self.h_size, self.h_size]) # reverse order\n",
    "        dJdW_rec = tf.transpose(dJdW_rec)\n",
    "        dJdW_out = tf.reshape(dJdW_out, [self.y_size, self.h_size]) # reverse order\n",
    "        dJdW_out = tf.transpose(dJdW_out)\n",
    "        \n",
    "        return [dJdW_in, dJdW_rec, dJdW_out]\n",
    "    \n",
    "    def _dydh(self):\n",
    "        return self.W_out\n",
    "\n",
    "    def _dhdz(self, h):\n",
    "        return tf.eye(self.h_size) * (1.0 - h**2)\n",
    "\n",
    "    def _dzdh(self):\n",
    "        return self.W_rec\n",
    "\n",
    "    def _dzdx(self):\n",
    "        return self.W_in\n",
    "\n",
    "    \n",
    "    # All _dzdW must unroll W to a vector [W11, ..., W1n, W21, ..., W2n, ..., Wmn]\n",
    "    # in order to compute the Jacobian [[dz1dW11, ..., dz1dWmn],\n",
    "    #                                   [dz2dW11, ..., dz2dWmn],\n",
    "    #                                   ...\n",
    "    #                                   [dzndW11, ..., dzndWmn]]\n",
    "    # The functions below actually return the transpose of the Jacobian for ease\n",
    "    # of calculation.\n",
    "    def _dzdW_in(self, t):\n",
    "        # Item assignment not supported in tf (unlike numpy) :-(\n",
    "        #dW_in = tf.zeros([self.h_size, self.x_size, self.h_size])\n",
    "        #dW_in[tf.range(self.h_size), :, tf.range(h_size)] = self.x[t] # not supported in tf\n",
    "        # So instead must use (loop + stack) x 2\n",
    "        dW_in = []\n",
    "        for i in range(self.h_size):\n",
    "            dW_in_i = []\n",
    "            for j in range(self.h_size):\n",
    "                if i == j:\n",
    "                    dW_in_i.append(tf.ones([self.x_size]) * tf.squeeze(self.x[t]))\n",
    "                else:\n",
    "                    dW_in_i.append(tf.zeros([self.x_size]))\n",
    "            dW_in.append(tf.stack(dW_in_i, axis=1))\n",
    "        dW_in = tf.stack(dW_in, axis=0)\n",
    "        #dW_in = tf.transpose(dW_in, [0, 2, 1])\n",
    "        return tf.reshape(dW_in, [-1, self.h_size])\n",
    "\n",
    "    def _dzdW_rec(self, t):\n",
    "        # Same indexing problem as explained in _dzdW_in\n",
    "        #dW_rec = tf.zeros([self.h_size, self.h_size, self.h_size])\n",
    "        #dW_rec[tf.range(self.h_size), :, tf.range(self.h_size)] = self.h[t-1]\n",
    "        dW_rec = []\n",
    "        if t > 0:\n",
    "            h_prev = self.h[t-1]\n",
    "        else:\n",
    "            h_prev = self.h_init\n",
    "        for i in range(self.h_size):\n",
    "            dW_rec_i = []\n",
    "            for j in range(self.h_size):\n",
    "                if i == j:\n",
    "                    dW_rec_i.append(tf.ones([self.h_size]) * tf.squeeze(h_prev))\n",
    "                else:\n",
    "                    dW_rec_i.append(tf.zeros([self.h_size]))\n",
    "            dW_rec.append(tf.stack(dW_rec_i, axis=1))\n",
    "        dW_rec = tf.stack(dW_rec, axis=0)\n",
    "        #dW_rec = tf.transpose(dW_rec, [0, 2, 1])\n",
    "        return tf.reshape(dW_rec, [-1, self.h_size])\n",
    "\n",
    "    def _dydW_out(self, t):\n",
    "        # Same indexing problem as explained in _dzdW_in\n",
    "        #dW_out = tf.zeros([self.y_size, self.h_size, self.y_size])\n",
    "        #dW_out[tf.range(self.y_size), :, tf.range(self.y_size)] = self.h[t]\n",
    "        dW_out = []\n",
    "        for i in range(self.y_size):\n",
    "            dW_out_i = []\n",
    "            for j in range(self.y_size):\n",
    "                if i == j:\n",
    "                    dW_out_i.append(tf.ones([self.h_size]) * tf.squeeze(self.h[t]))\n",
    "                else:\n",
    "                    dW_out_i.append(tf.zeros([self.h_size]))\n",
    "            dW_out.append(tf.stack(dW_out_i, axis=1))\n",
    "        dW_out = tf.stack(dW_out, axis=0)\n",
    "        #dW_out = tf.transpose(dW_out, [0, 2, 1])\n",
    "        return tf.reshape(dW_out, [-1, self.y_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[[ 0.471 -1.191  1.433]]\n",
      "\n",
      " [[-0.313 -0.721  0.887]]\n",
      "\n",
      " [[ 0.86  -0.637  0.016]]]\n",
      "\n",
      "h\n",
      "[[ 0.978 -0.826 -0.953]]\n",
      "[[ 0.98  -0.722 -0.278]]\n",
      "[[ 0.966  0.991 -0.003]]\n",
      "\n",
      "y\n",
      "[[1.433 2.553]]\n",
      "[[1.731 1.62 ]]\n",
      "[[ 0.16  -3.197]]\n",
      "\n",
      "J\n",
      "4.299705\n"
     ]
    }
   ],
   "source": [
    "# Set network settings\n",
    "trace_length = 3\n",
    "x_size = 3\n",
    "h_size = 3\n",
    "y_size = 2\n",
    "seed = 1234\n",
    "rnn = StaticRNN(trace_length, x_size, h_size, y_size, seed=seed)\n",
    "\n",
    "# Initialize tf session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Create (deterministic) input state\n",
    "r = np.random.RandomState(seed)\n",
    "x_ = r.randn(trace_length, 1, x_size)\n",
    "feed_dict = {rnn.x: x_}\n",
    "\n",
    "# Get intermediate and output values\n",
    "h_, y_, J_ = sess.run([rnn.h, rnn.y, rnn.J], feed_dict=feed_dict)\n",
    "\n",
    "print(\"x\")\n",
    "print(x_)\n",
    "print()\n",
    "\n",
    "print(\"h\")\n",
    "for h_i in h_:\n",
    "    print(h_i)\n",
    "print()\n",
    "\n",
    "print(\"y\")\n",
    "for y_i in y_:\n",
    "    print(y_i)\n",
    "print()\n",
    "\n",
    "print(\"J\")\n",
    "print(J_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.124  0.116 -0.207]\n",
      " [-0.227  2.051  0.911]\n",
      " [ 0.17  -2.449 -0.821]]\n",
      "[[ 0.22  -1.653 -1.011]\n",
      " [-0.171  1.389  0.813]\n",
      " [-0.12   1.564  0.724]]\n",
      "[[ 2.925  2.925]\n",
      " [-0.558 -0.558]\n",
      " [-1.233 -1.233]]\n",
      "\n",
      "[[ 0.124  0.116 -0.207]\n",
      " [-0.227  2.051  0.911]\n",
      " [ 0.17  -2.449 -0.821]]\n",
      "[[ 0.22  -1.653 -1.011]\n",
      " [-0.171  1.389  0.813]\n",
      " [-0.12   1.564  0.724]]\n",
      "[[ 2.925  2.925]\n",
      " [-0.558 -0.558]\n",
      " [-1.233 -1.233]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate gradients automatically\n",
    "auto_grads = sess.run(rnn.auto_grads, feed_dict=feed_dict)\n",
    "for g in auto_grads:\n",
    "    print(g[0])\n",
    "print()\n",
    "\n",
    "# Calculate gradients manually\n",
    "manual_grads = sess.run(rnn.manual_grads, feed_dict=feed_dict)\n",
    "for g in manual_grads:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-order gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto hessian:\n",
      "[[-2.4052e-01 -1.3991e-02 -6.8192e-03  1.8270e-01  3.9125e-03 -2.0616e-02\n",
      "  -4.5936e-02 -3.4976e-03  2.5305e-02]\n",
      " [-1.3991e-02 -5.3603e-01 -6.9091e-03 -1.3071e-02  1.5965e-01 -1.4832e-01\n",
      "   3.1889e-02 -2.6690e-01  1.8075e-01]\n",
      " [-6.8192e-03 -6.9091e-03 -9.8862e-02  5.6398e-03  6.3765e-02  1.1305e-01\n",
      "  -1.3975e-03 -6.0784e-02 -1.3318e-01]\n",
      " [ 1.8270e-01 -1.3071e-02  5.6398e-03 -3.8489e-01  2.2344e-03  1.6539e-02\n",
      "   3.3489e-01 -1.8561e-03 -2.0296e-02]\n",
      " [ 3.9125e-03  1.5965e-01  6.3765e-02  2.2344e-03 -2.2147e+00  1.7141e-02\n",
      "   7.9934e-03  2.7224e+00 -2.2553e-02]\n",
      " [-2.0616e-02 -1.4832e-01  1.1305e-01  1.6539e-02  1.7141e-02 -6.7106e-01\n",
      "   7.1318e-06  2.6350e-02  8.1085e-01]\n",
      " [-4.5936e-02  3.1889e-02 -1.3975e-03  3.3489e-01  7.9934e-03  7.1320e-06\n",
      "  -4.0060e-01 -1.1342e-02  2.1263e-05]\n",
      " [-3.4976e-03 -2.6690e-01 -6.0784e-02 -1.8561e-03  2.7224e+00  2.6350e-02\n",
      "  -1.1342e-02 -3.2687e+00 -3.0556e-02]\n",
      " [ 2.5305e-02  1.8075e-01 -1.3318e-01 -2.0296e-02 -2.2553e-02  8.1085e-01\n",
      "   2.1264e-05 -3.0556e-02 -9.8095e-01]]\n",
      "\n",
      "numerical hessian:\n",
      "[[-2.4012e-01 -1.4066e-02 -7.0327e-03  1.8281e-01  3.4716e-03 -2.0465e-02\n",
      "  -4.6047e-02 -3.6226e-03  2.5430e-02]\n",
      " [-1.3254e-02 -5.3606e-01 -6.8214e-03 -1.4686e-02  1.6063e-01 -1.4871e-01\n",
      "   3.1968e-02 -2.6641e-01  1.8093e-01]\n",
      " [-7.0131e-03 -6.9634e-03 -9.8790e-02  5.8133e-03  6.3635e-02  1.1289e-01\n",
      "  -1.5170e-03 -6.0734e-02 -1.3293e-01]\n",
      " [ 1.8170e-01 -1.2972e-02  5.6443e-03 -3.8612e-01  2.1474e-03  1.6640e-02\n",
      "   3.3448e-01 -1.6816e-03 -2.0344e-02]\n",
      " [ 3.0883e-03  1.5980e-01  6.3747e-02  0.0000e+00 -2.2150e+00  1.6831e-02\n",
      "   8.2880e-03  2.7218e+00 -2.2306e-02]\n",
      " [-2.1361e-02 -1.4850e-01  1.1289e-01  1.5910e-02  1.6320e-02 -6.7095e-01\n",
      "  -1.4800e-04  2.6691e-02  8.1088e-01]\n",
      " [-4.4781e-02  3.1892e-02 -1.2979e-03  3.3686e-01  8.4465e-03 -1.9126e-04\n",
      "  -3.9997e-01 -1.1449e-02 -2.5817e-05]\n",
      " [-2.0589e-03 -2.6708e-01 -6.1332e-02  4.8954e-03  2.7212e+00  2.6777e-02\n",
      "  -1.1248e-02 -3.2679e+00 -3.0568e-02]\n",
      " [ 2.5736e-02  1.8097e-01 -1.3281e-01 -2.0805e-02 -2.2047e-02  8.1019e-01\n",
      "   2.9600e-04 -3.0841e-02 -9.8076e-01]]\n",
      "\n",
      "Auto hessian NOT verified\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Auto hessian\n",
    "d2JdW_in2 = true_hessian(rnn.J, rnn.W_in)\n",
    "H_auto = sess.run(d2JdW_in2, feed_dict=feed_dict)\n",
    "print(\"auto hessian:\")\n",
    "print(H_auto)\n",
    "print()\n",
    "\n",
    "# Numerical estimation (brute force)\n",
    "feed_dict = {rnn.x: x_}\n",
    "dJdW_in = sess.run(rnn.auto_grads[0][0], feed_dict=feed_dict).flatten()\n",
    "N = len(dJdW_in)\n",
    "H_num = np.zeros([N, N])\n",
    "epsilon = 5e-4\n",
    "W_in = sess.run(rnn.W_in)\n",
    "W_in_shape = W_in.shape\n",
    "W_in = W_in.flatten()\n",
    "for j in range(N):\n",
    "    W_in_ = copy(W_in)\n",
    "    delta = epsilon * W_in_[j]\n",
    "    W_in_[j] += delta\n",
    "    W_in_ = np.reshape(W_in_, W_in_shape)\n",
    "    feed_dict = {rnn.x: x_, rnn.W_in: W_in_}\n",
    "    dJdW_in_ = sess.run(rnn.auto_grads[0][0], feed_dict=feed_dict).flatten()\n",
    "    H_num[:, j] = (dJdW_in_ - dJdW_in) / delta\n",
    "print(\"numerical hessian:\")\n",
    "print(H_num)\n",
    "print()\n",
    "\n",
    "# This is pretty difficult due to the balance between precision of estimate (small ε)\n",
    "# and numerical stability (large ε)\n",
    "is_close = np.allclose(H_auto, H_num)\n",
    "print(\"Auto hessian {}verified\".format(\"\" if is_close else \"NOT \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian-free optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StaticRNN_HF(StaticRNN):\n",
    "    def __init__(self, trace_length, x_size, h_size, y_size, seed=1234):\n",
    "        # Initialize base class\n",
    "        StaticRNN.__init__(self, trace_length, x_size, h_size, y_size, seed=seed)\n",
    "        \n",
    "        # Build loss function\n",
    "        self.y_target = tf.placeholder(tf.float32, \n",
    "                                       shape=[trace_length, y_size],\n",
    "                                       name='y_target')\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.square(self.y - self.y_target), name=\"mse_loss\")\n",
    "    \n",
    "        # Build auto gradients\n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            self.grads = tf.gradients(self.loss, [self.W, self.b])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = StaticRNN_HF(trace_length, x_size, h_size, y_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's figure out how to compute the matrix-vector products $G_f v$, where $G_f$ is the Gauss-Newton matrix and $v$ is the direction. The Gauss-Newton matrix for a single training example is $G_f = J_{s,\\theta}^{T} (L \\circ g)'' J_{s,\\theta} \\big|_{\\theta = \\theta_n}$. First, we will compute $J_{s,\\theta} v$ using the method in [Pearlmutter, 1994]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old gradient code\n",
    "Outside StaticRNN class, so get to use numpy :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using derivations from this [blog post](http://willwolf.io/2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_in_ = sess.run(rnn.W_in)\n",
    "W_rec_ = sess.run(rnn.W_rec)\n",
    "W_out_ = sess.run(rnn.W_out)\n",
    "\n",
    "def dydh():\n",
    "    return W_out_\n",
    "\n",
    "def dhdz(h):\n",
    "    return np.eye(h_size) * (1.0 - h**2)\n",
    "\n",
    "def dzdh():\n",
    "    return W_rec_\n",
    "\n",
    "def dzdx():\n",
    "    return W_in_\n",
    "\n",
    "def dzdW_in(t):\n",
    "    # Need to vectorize W_in in order for matmul to work in dhdz @ dzdW_in\n",
    "    dW_in = np.zeros([h_size, x_size, h_size])\n",
    "    dW_in[np.arange(h_size), :, np.arange(h_size)] = x_[t]\n",
    "    #dW_in = np.transpose(dW_in, [0, 2, 1])\n",
    "    return dW_in.reshape([h_size, -1])\n",
    "\n",
    "def dzdW_rec(t):\n",
    "    # Need to vectorize W_in in order for matmul to work in dhdz @ dzdW_in\n",
    "    dW_rec = np.zeros([h_size, h_size, h_size])\n",
    "    dW_rec[np.arange(h_size), :, np.arange(h_size)] = h_[t-1]\n",
    "    #dW_rec = np.transpose(dW_rec, [0, 2, 1])\n",
    "    return dW_rec.reshape([h_size, -1])\n",
    "\n",
    "def dydW_out(t):\n",
    "    dW_out = np.zeros([y_size, h_size, y_size])\n",
    "    dW_out[np.arange(y_size), :, np.arange(y_size)] = h_[t]\n",
    "    #dW_out = np.transpose(dW_out, [0, 2, 1])\n",
    "    return dW_out.reshape([y_size, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.043 -0.546 -0.017]\n",
      " [-0.108  1.379  0.042]\n",
      " [ 0.129 -1.659 -0.051]]\n",
      "[[ 0.978  0.978]\n",
      " [-0.826 -0.826]\n",
      " [-0.953 -0.953]]\n"
     ]
    }
   ],
   "source": [
    "dJdW_in = []\n",
    "dJdW_rec = []\n",
    "dJdW_out = []\n",
    "\n",
    "for t in range(trace_length)[::-1]:\n",
    "    # Compute dJdW_in = Σ(dJdy * dydh * dhdz * dzdW_in) for [0, t]\n",
    "    dJ_tdW_in = 0\n",
    "    for j in range(t+1)[::-1]:\n",
    "        dJ_tdy_t = np.ones([1, y_size])\n",
    "        dy_tdh_t = dydh()\n",
    "        #dh_tdz_j = np.ones([1, h_size])\n",
    "        dh_tdz_j = dhdz(h_[j])\n",
    "        for k in range(j)[::-1]:\n",
    "            #print(dhdz(h_[k]) )\n",
    "            dh_tdz_j *= (dhdz(h_[k]) @ dzdh())\n",
    "        dz_jdW_in = dzdW_in(j)\n",
    "        #print(dJ_tdy_t.shape, dy_tdh_t.shape, dh_tdz_j.shape, dz_jdW_in.shape)\n",
    "        dJ_tdW_in += ((dJ_tdy_t @ dy_tdh_t.T) @ dh_tdz_j.T) @ dz_jdW_in\n",
    "    dJdW_in.append(dJ_tdW_in)\n",
    "    \n",
    "    # Compute dydW_rec = Σ(dydh * dhdz * dzdW_rec) for [0, t]\n",
    "    dJ_tdW_rec = 0\n",
    "    for j in range(t)[::-1]:\n",
    "        dJ_tdy_t = np.ones([1, y_size])\n",
    "        dy_tdh_t = dydh()\n",
    "        dh_tdz_j = np.ones([1, h_size])\n",
    "        for k in range(j)[::-1]:\n",
    "            dh_tdz_j *= (dhdz(h_[k]) @ dzdh())\n",
    "        dz_jdW_rec = dzdW_rec(j)\n",
    "        dJ_tdW_rec += ((dJ_tdy_t @ dy_tdh_t.T) @ dh_tdz_j.T) @ dz_jdW_rec\n",
    "    dJdW_rec.append(dJ_tdW_rec)\n",
    "    \n",
    "    # Compute dJdW_in = Σ(dJdy * dydh * dhdz * dzdW_in) for [0, t]\n",
    "    dJ_tdW_out = 0\n",
    "    for j in range(t+1)[::-1]:\n",
    "        dJ_tdy_t = np.ones([1, y_size])\n",
    "        dy_tdW_out = dydW_out(j)\n",
    "        dJ_tdW_out += dJ_tdy_t @ dy_tdW_out\n",
    "    dJdW_out.append(dJ_tdW_out)\n",
    "\n",
    "print(dJdW_in[0].reshape([x_size, h_size]))\n",
    "# throws error with trace_length=1 because inner loop not accessed\n",
    "#print(dJdW_rec[0].reshape([h_size, h_size]))\n",
    "print(dJdW_out[0].reshape([h_size, y_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.008  0.037  0.059]\n",
      " [-0.021 -0.094 -0.148]\n",
      " [ 0.026  0.113  0.179]]\n",
      "[[0.906 0.906]\n",
      " [0.918 0.918]\n",
      " [0.928 0.928]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
