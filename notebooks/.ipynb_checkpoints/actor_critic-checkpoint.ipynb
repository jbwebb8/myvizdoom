{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../python', '', '/home/james/anaconda3/envs/vizdoom/lib/python36.zip', '/home/james/anaconda3/envs/vizdoom/lib/python3.6', '/home/james/anaconda3/envs/vizdoom/lib/python3.6/lib-dynload', '/home/james/.local/lib/python3.6/site-packages', '/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages', '/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/Sphinx-1.5.4-py3.6.egg', '/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg', '/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/extensions', '/home/james/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../python\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "from helper import create_agent\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If running other experiments on GPUs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializes DoomGame from config file\n",
    "def initialize_vizdoom(config_file):\n",
    "    game = DoomGame()\n",
    "    game.load_config(config_file)\n",
    "    game.init()\n",
    "    return game  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize agent and TensorFlow graph\n",
    "def make_new_agent():\n",
    "    tf.reset_default_graph()\n",
    "    agent_file_path = \"./actor_critic/ac.json\"\n",
    "    config_file_path = \"./actor_critic/ac.cfg\"\n",
    "    results_dir = \"../tmp/tmp_results/\"\n",
    "    action_set = \"basic_three\"\n",
    "    game = initialize_vizdoom(config_file_path)\n",
    "    return create_agent(agent_file_path,\n",
    "                        game=game, \n",
    "                        action_set=action_set,\n",
    "                        output_directory=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-step learning\n",
    "First, we will walk through the n-step learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prints status of memory buffers\n",
    "def print_agent_status():\n",
    "    print(\"s1_buffer:    \\n\", agent.s1_buffer[:, :5, 0, -1])\n",
    "    print(\"a_buffer:     \\n\", agent.a_buffer)\n",
    "    print(\"s2_buffer:    \\n\", agent.s2_buffer[:, :5, 0, -1])\n",
    "    print(\"r_buffer:     \\n\", agent.r_buffer)\n",
    "    print(\"gamma_buffer: \\n\", agent.gamma_buffer)\n",
    "    print(\"memory r:     \\n\", agent.memory.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: \n",
      "s1_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a_buffer:     \n",
      " [2 0 0 0 0]\n",
      "s2_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "r_buffer:     \n",
      " [-0.04  0.    0.    0.    0.  ]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Step 2: \n",
      "s1_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a_buffer:     \n",
      " [2 2 0 0 0]\n",
      "s2_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04  0.    0.    0.  ]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Step 3: \n",
      "s1_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a_buffer:     \n",
      " [2 2 2 0 0]\n",
      "s2_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04  0.    0.  ]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Step 4: \n",
      "s1_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a_buffer:     \n",
      " [2 2 2 1 0]\n",
      "s2_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04  0.  ]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Step 5: \n",
      "s1_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]]\n",
      "a_buffer:     \n",
      " [2 2 2 1 1]\n",
      "s2_buffer:    \n",
      " [[ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 0.96059601  1.          0.99        0.9801      0.970299  ]\n",
      "memory r:     \n",
      " [ 0.02110773  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "\n",
      "V(s):  [[ 0.22833839]]\n"
     ]
    }
   ],
   "source": [
    "# View memory storage\n",
    "agent = make_new_agent()\n",
    "agent.initialize_new_episode()\n",
    "for i in range(5):\n",
    "    print(\"Step %d: \" % (i+1))\n",
    "    agent.perform_learning_step(1, 1)\n",
    "    print_agent_status()\n",
    "    print()\n",
    "print(\"V(s): \", agent.network.get_value_output(agent.s2_buffer[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, during the first $n$ steps, the agent stores transitions consisting of $s1, a, s2, r$. After the $n$th step, the return $R$ can be calculated for time $t-n$. The first $Q(s,a) \\approx \\mathbb{E}[R_t]$ is given by:\n",
    "\n",
    "$\\sum_{i=0}^{k-1}(\\gamma^ir_i)+\\gamma^kV(s_t)$\n",
    "\n",
    "which in this case is vectorized:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "1.0\n",
    "\\\\ 0.99\n",
    "\\\\ 0.9801\n",
    "\\\\ 0.970299\n",
    "\\\\ 0.96059601\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "-0.04\n",
    "\\\\ -0.04\n",
    "\\\\ -0.04\n",
    "\\\\ -0.04\n",
    "\\\\ -0.04\n",
    "\\end{bmatrix}\n",
    "+ 0.99^5V(s_5) = -0.196 + (0.951)(0.228338) = 0.0211$\n",
    "\n",
    "which matches the initial return placed in memory. The $s1, s2, a, r$ buffers roll to the beginning, placing the next transition at slot 0, while the $\\gamma$ buffer rolls forward by one slot to match the new configuration. Now for the next five steps, we should see a transition added after each learning step, updating the $r$ array in memory as calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: \n",
      "s1_buffer:    \n",
      " [[ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]]\n",
      "a_buffer:     \n",
      " [1 2 2 1 1]\n",
      "s2_buffer:    \n",
      " [[ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 0.970299    0.96059601  1.          0.99        0.9801    ]\n",
      "memory r:     \n",
      " [ 0.02110773  0.03489357  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "V(s):  [[ 0.24283469]] \n",
      "\n",
      "Step 7: \n",
      "s1_buffer:    \n",
      " [[ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.47058824  0.46568626  0.47058824  0.47058824  0.41549021]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]]\n",
      "a_buffer:     \n",
      " [1 0 2 1 1]\n",
      "s2_buffer:    \n",
      " [[ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.49019608  0.56078434  0.49019608  0.47058824  0.34901962]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 0.9801      0.970299    0.96059601  1.          0.99      ]\n",
      "memory r:     \n",
      " [ 0.02110773  0.03489357 -0.04599142  0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "V(s):  [[ 0.15778123]] \n",
      "\n",
      "Step 8: \n",
      "s1_buffer:    \n",
      " [[ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.49019608  0.56078434  0.49019608  0.47058824  0.34901962]\n",
      " [ 0.49990195  0.4509804   0.47058824  0.44549018  0.40000001]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]]\n",
      "a_buffer:     \n",
      " [1 0 0 1 1]\n",
      "s2_buffer:    \n",
      " [[ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.49019608  0.56078434  0.49019608  0.47058824  0.34901962]\n",
      " [ 0.41470587  0.41617647  0.43823528  0.43656862  0.3617647 ]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 0.99        0.9801      0.970299    0.96059601  1.        ]\n",
      "memory r:     \n",
      " [ 0.02110773  0.03489357 -0.04599142  0.05962786  0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "V(s):  [[ 0.26884368]] \n",
      "\n",
      "Step 9: \n",
      "s1_buffer:    \n",
      " [[ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.49019608  0.56078434  0.49019608  0.47058824  0.34901962]\n",
      " [ 0.41470587  0.41617647  0.43823528  0.43656862  0.3617647 ]\n",
      " [ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]]\n",
      "a_buffer:     \n",
      " [1 0 0 1 1]\n",
      "s2_buffer:    \n",
      " [[ 0.49225491  0.43235293  0.41127452  0.49117646  0.59019607]\n",
      " [ 0.49019608  0.56078434  0.49019608  0.47058824  0.34901962]\n",
      " [ 0.41470587  0.41617647  0.43823528  0.43656862  0.3617647 ]\n",
      " [ 0.40000001  0.46620914  0.43176469  0.40771243  0.39267975]\n",
      " [ 0.4509804   0.37450981  0.41274509  0.62352943  0.54509807]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.02110773  0.03489357 -0.04599142  0.05962786  0.03615659  0.          0.\n",
      "  0.          0.          0.        ]\n",
      "V(s):  [[ 0.2441628]] \n",
      "\n",
      "Step 10: \n",
      "s1_buffer:    \n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "a_buffer:     \n",
      " [0 0 0 0 0]\n",
      "s2_buffer:    \n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "r_buffer:     \n",
      " [ 0.  0.  0.  0.  0.]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [ 0.02110773  0.03489357 -0.04599142  0.05962786  0.03615659 -0.04       -0.0796\n",
      " -0.118804   -0.15761596 -0.1960398 ]\n",
      "V(s):  [[ 0.02561042]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now view adding transition to memory\n",
    "for i in range(5):\n",
    "    print(\"Step %d: \" % (i+6))\n",
    "    agent.perform_learning_step(1, 1)\n",
    "    print_agent_status()\n",
    "    print(\"V(s): \", agent.network.get_value_output(agent.s2_buffer[i]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next four transitions, the returns are calculated as above. For example, in step 6, the return is:\n",
    "\n",
    "$R_2 = -1.96 + (0.951)(0.242835) = 0.0349$\n",
    "\n",
    "Note that the $\\gamma$ buffer is displayed after it has been updated; thus, the buffer shown for the previous step represents the buffer used in the current calculation. Additionally, because the episode times out after 10 actions (40 tics), during step 10, the agent encounters a terminal state in s2; thus after calculating the last $n$ transitions, the buffers are reset, which is displayed in step 10. The returns for the last $n$ states are given as:\n",
    "\n",
    "$R_{T-i}=\\sum_{i=0}^{n} \\gamma^i r_{t-i}$\n",
    "\n",
    "or recursively:\n",
    "\n",
    "$R_t \\leftarrow r_t + \\gamma R_{t+1}$\n",
    "\n",
    "which in this case, because all rewards were $-0.04$, is simply:\n",
    "\n",
    "$R_{T-i}=\\sum_{i=0}^{n} \\gamma^i (-0.04) = (-0.04)\\sum_{i=0}^{n} \\gamma^i$\n",
    "\n",
    "Due to the recursive formula, the transitions are added in reverse order, as seen in the last 5 slots of the memory matrix $r$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing transitions into memory\n",
    "Now that we know the agent is properly calculating the n-step return of states, we need to make sure that the other variables ($s1, a, s2, isterminal$) are being properly associated with these returns in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_memory_status():\n",
    "    print(\"s1: \", agent.memory.s1[:, :5, 0, -1]) # due to storage of overlapping states\n",
    "    print(\"a:  \", agent.memory.a)\n",
    "    print(\"s2: \", agent.memory.s2[:, :5, 0, 0])\n",
    "    print(\"R:  \", agent.memory.r)\n",
    "    print(\"isterminal: \", agent.memory.isterminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 45, 4)\n",
      "(10, 30, 45, 1)\n",
      "s1_buffer:    \n",
      " [[ 0.46557733  0.48518518  0.50230938  0.47058824  0.42763618]\n",
      " [ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]]\n",
      "a_buffer:     \n",
      " [0 1 2 1 2]\n",
      "s2_buffer:    \n",
      " [[ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]\n",
      " [ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 0.96059601  1.          0.99        0.9801      0.970299  ]\n",
      "memory r:     \n",
      " [-0.17756607  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "s1:  [[ 0.46557733  0.48518518  0.50230938  0.47058824  0.42763618]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a:   [0 0 0 0 0 0 0 0 0 0]\n",
      "s2:  [[ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "R:   [-0.17756607  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "isterminal:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Load memory with transitions\n",
    "agent = make_new_agent()\n",
    "agent.initialize_new_episode()\n",
    "print(agent.memory.s1.shape)\n",
    "print(agent.memory.s2.shape)\n",
    "for i in range(5):\n",
    "    agent.perform_learning_step(1, 1)\n",
    "\n",
    "# Compare current status with replay memory\n",
    "print_agent_status()\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the next transitions $(t,...,T-2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_buffer:    \n",
      " [[ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]\n",
      " [ 0.45473856  0.51831156  0.4509804   0.4509804   0.34901962]\n",
      " [ 0.47058824  0.48962963  0.48007625  0.4516122   0.3685621 ]\n",
      " [ 0.5260784   0.52156866  0.40000001  0.52156866  0.54509807]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]]\n",
      "a_buffer:     \n",
      " [0 0 0 2 2]\n",
      "s2_buffer:    \n",
      " [[ 0.45473856  0.51831156  0.4509804   0.4509804   0.34901962]\n",
      " [ 0.47058824  0.48962963  0.48007625  0.4516122   0.3685621 ]\n",
      " [ 0.5260784   0.52156866  0.40000001  0.52156866  0.54509807]\n",
      " [ 0.44149238  0.40000001  0.69411767  0.54509807  0.47058824]\n",
      " [ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]]\n",
      "r_buffer:     \n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [-0.17756607 -0.16151434 -0.11614615 -0.15517615 -0.1869307   0.          0.\n",
      "  0.          0.          0.        ]\n",
      "s1:  [[ 0.46557733  0.48518518  0.50230938  0.47058824  0.42763618]\n",
      " [ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "a:   [0 1 2 1 2 0 0 0 0 0]\n",
      "s2:  [[ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]\n",
      " [ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "R:   [-0.17756607 -0.16151434 -0.11614615 -0.15517615 -0.1869307   0.          0.\n",
      "  0.          0.          0.        ]\n",
      "isterminal:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    agent.perform_learning_step(1, 1)\n",
    "\n",
    "# Compare current status with replay memory\n",
    "print_agent_status()\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally to the terminal step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_buffer:    \n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "a_buffer:     \n",
      " [0 0 0 0 0]\n",
      "s2_buffer:    \n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "r_buffer:     \n",
      " [ 0.  0.  0.  0.  0.]\n",
      "gamma_buffer: \n",
      " [ 1.          0.99        0.9801      0.970299    0.96059601]\n",
      "memory r:     \n",
      " [-0.17756607 -0.16151434 -0.11614615 -0.15517615 -0.1869307  -0.04       -0.0796\n",
      " -0.118804   -0.15761596 -0.1960398 ]\n",
      "s1:  [[ 0.46557733  0.48518518  0.50230938  0.47058824  0.42763618]\n",
      " [ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]\n",
      " [ 0.44149238  0.40000001  0.69411767  0.54509807  0.47058824]\n",
      " [ 0.5260784   0.52156866  0.40000001  0.52156866  0.54509807]\n",
      " [ 0.47058824  0.48962963  0.48007625  0.4516122   0.3685621 ]\n",
      " [ 0.45473856  0.51831156  0.4509804   0.4509804   0.34901962]\n",
      " [ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]]\n",
      "a:   [0 1 2 1 2 2 2 0 0 0]\n",
      "s2:  [[ 0.4559913   0.47058824  0.47058824  0.45254901  0.43845317]\n",
      " [ 0.40000001  0.40000001  0.40000001  0.40000001  0.40000001]\n",
      " [ 0.48007625  0.51218957  0.52156866  0.47058824  0.34901962]\n",
      " [ 0.39523965  0.41459695  0.40293029  0.36862746  0.39198259]\n",
      " [ 0.52156866  0.55385619  0.47503269  0.4509804   0.40000001]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.44149238  0.40000001  0.69411767  0.54509807  0.47058824]\n",
      " [ 0.5260784   0.52156866  0.40000001  0.52156866  0.54509807]\n",
      " [ 0.47058824  0.48962963  0.48007625  0.4516122   0.3685621 ]\n",
      " [ 0.45473856  0.51831156  0.4509804   0.4509804   0.34901962]]\n",
      "R:   [-0.17756607 -0.16151434 -0.11614615 -0.15517615 -0.1869307  -0.04       -0.0796\n",
      " -0.118804   -0.15761596 -0.1960398 ]\n",
      "isterminal:  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Perform terminal learning step\n",
    "agent.perform_learning_step(1, 1)\n",
    "print_agent_status()\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and value functions\n",
    "Next, we will look into the policy and value functions, analyzing both their output and loss functions, as well as the gradients derived from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_learning_step():\n",
    "    s1, a, s2, isterminal, R, w = agent.memory.get_sample(1)\n",
    "    print(\"s1: \", s1[:, :3, 1, 1])\n",
    "    print(\"a:  \", a)\n",
    "    print(\"R:  \", R)\n",
    "    print(\"w:  \", w)\n",
    "    print(\"V:  \", agent.network.get_value_output(s1))\n",
    "    print(\"pi: \", agent.network.get_policy_output(s1))\n",
    "    loss_pi, loss_v = agent.network.learn(s1, a, R, weights=w)\n",
    "    print(\"loss_pi: \", loss_pi, \" loss_v: \", loss_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:  [[ 0.47058824  0.46568626  0.47058824]]\n",
      "a:   [1]\n",
      "R:   [-0.31398159]\n",
      "w:   [ 1.]\n",
      "V:   [[-0.11301116]]\n",
      "pi:  [[ 0.23030756  0.27955541  0.49013704]]\n",
      "loss_pi:  [[-0.26658764]]  loss_v:  0.0403891\n"
     ]
    }
   ],
   "source": [
    "# Load memory with transitions\n",
    "agent = make_new_agent()\n",
    "agent.initialize_new_episode()\n",
    "for i in range(10):\n",
    "    agent.perform_learning_step(1, 1)\n",
    "print_learning_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the loss calculations. The loss of the policy is given by:\n",
    "\n",
    "$L_{\\pi} = -log(\\pi(a_t|s_t))(R_t-V(s_t)) - \\beta H(\\pi(s_t))$\n",
    "\n",
    "where the entropy is $\\sum_{i}-\\pi(a_i|s_t)log(\\pi(a_i|s_t))$. For this transition, this becomes:\n",
    "\n",
    "$L_{\\pi} = −log(0.2796)(−0.3140−(−0.1130))+(0.01)((0.2303)log(0.2303) + (0.2796)log(0.2796) + (0.4901)log(0.4901) = -0.2666$\n",
    "\n",
    "The loss of the value function is simpler, as it just calculates the (mean) squared error:\n",
    "\n",
    "$L_{V} = \\sum_{i} (R_t - V(s_t))^2$\n",
    "\n",
    "which in this case becomes:\n",
    "\n",
    "$L_{V} = (-0.3140 - (-0.1130))^2 = 0.0404$\n",
    "\n",
    "Let's redefine the previous function to now include gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_learning_step():\n",
    "    s1, a, s2, isterminal, R, w = agent.memory.get_sample(1)\n",
    "    s1 = agent.network._check_state(s1)\n",
    "    a = agent.network._check_actions(a)\n",
    "    if w is None:\n",
    "        w = np.ones(a.shape[0])\n",
    "    print(\"s1: \", s1[:, :3, 1, 1])\n",
    "    print(\"a:  \", a)\n",
    "    print(\"R:  \", R)\n",
    "    print(\"w:  \", w)\n",
    "    print(\"V:  \", agent.network.get_value_output(s1))\n",
    "    print(\"pi: \", agent.network.get_policy_output(s1))\n",
    "    opt = agent.network.optimizer\n",
    "    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                 scope=agent.network.scope)\n",
    "    fd = {agent.network.state: s1, agent.network.actions: a, \n",
    "          agent.network.q_sa: R, agent.network.IS_weights: w}\n",
    "    sess = agent.network.sess\n",
    "    gvs_pi = opt.compute_gradients(agent.network.loss_pi, var_list=var_list)\n",
    "    grads_pi = sess.run([g for g, v in gvs_pi], feed_dict=fd)\n",
    "    gvs_v = opt.compute_gradients(agent.network.loss_v, var_list=var_list)\n",
    "    gvs_v = [[g, v] for g, v in gvs_v if g is not None]\n",
    "    grads_v = sess.run([g for g, v in gvs_v], feed_dict=fd)\n",
    "    loss_pi, loss_v = agent.network.learn(s1, a, R, weights=w)\n",
    "    print(\"loss_pi: \", loss_pi, \" loss_v: \", loss_v)\n",
    "    print(\"d(loss_pi)/d(pi): %s\" % gvs_pi[-1][1].name[:-2], grads_pi[-1])\n",
    "    print(\"d(loss_pi)/d(V): %s\" % gvs_pi[-3][1].name[:-2], grads_pi[-3])\n",
    "    print(\"d(loss_V)/d(V): %s\" % gvs_v[-1][1].name[:-2], grads_v[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:  [[ 0.4509804   0.37450981  0.41274509]]\n",
      "a:   [[0 2]]\n",
      "R:   [-0.04]\n",
      "w:   [ 1.]\n",
      "V:   [[ 0.20522182]]\n",
      "pi:  [[ 0.29009074  0.33439586  0.3755134 ]]\n",
      "loss_pi:  [[-0.25111637]]  loss_v:  0.0601337\n",
      "d(loss_pi)/d(pi): global_network/pi/biases [-0.07155561 -0.08200891  0.15356451]\n",
      "d(loss_pi)/d(V): global_network/V/biases [-0.97946107]\n",
      "d(loss_V)/d(V): global_network/V/biases [ 0.49044365]\n"
     ]
    }
   ],
   "source": [
    "# Load memory with transitions\n",
    "agent = make_new_agent()\n",
    "agent.initialize_new_episode()\n",
    "for i in range(10):\n",
    "    agent.perform_learning_step(1, 1)\n",
    "print_learning_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives of the loss functions are as follows:\n",
    "\n",
    "$\\dfrac{\\partial L_\\pi}{\\partial \\pi(a|s_t)} = \n",
    "\\left\\{\\begin{matrix}\n",
    "   -\\dfrac{R_t - V(s_t)}{\\pi(a|s_t)} + \\beta (1 + log\\pi(a|s_t)) \\text{ if } a = a_t \\\\ \n",
    "   \\beta (1 + log\\pi(a|s_t)) \\text{ if } a \\neq a_t\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "$\\dfrac{\\partial L_\\pi}{\\partial V(s_t)} = log\\pi(a_t|s_t)$\n",
    "\n",
    "$\\dfrac{\\partial L_V}{\\partial \\pi(a|s_t)} = 0$\n",
    "\n",
    "$\\dfrac{\\partial L_V}{\\partial V(s_t)} = -2(R_t - V(s_t))$\n",
    "\n",
    "Additionally, due to the softmax activation in the $\\pi$ output layer, the derivatives for its biases and weights must backpropagate through the softmax function. Further, because of the summation term ($\\sum_i e^f_i$) in softmax, the derivatives $\\frac{\\partial \\pi(a_i|s_t)}{\\partial w_{\\pi(a_j|s_t)}}$, where $i \\neq j$, are nonzero. Therefore:\n",
    "\n",
    "$\\dfrac{\\partial L_\\pi}{\\partial w_{\\pi(a_j|s_t)}}\n",
    "= \\dfrac{\\partial L_\\pi}{\\partial \\pi(s_t)} \\dfrac{\\partial \\pi(s_t)}{\\partial w_{\\pi(a_j|s_t)}}\n",
    "= \\sum_i \\left ( \\dfrac{\\partial L_\\pi}{\\partial \\pi(a_i|s_t)} \\dfrac{\\partial \\pi(a_i|s_t)}{\\partial w_{\\pi(a_j|s_t)}} \\right )$,\n",
    "\n",
    "where, for $f_k = b_{\\pi(a_k|s_t)} + w_{\\pi(a_k|s_t)}^{(1)}x^{(1)} + w_{\\pi(a_k|s_t)}^{(2)}x^{(2)} + \\cdots + w_{\\pi(a_k|s_t)}^{(n)}x^{(n)}$:\n",
    "\n",
    "$ \\dfrac{\\partial \\pi(a_i|s_t)}{\\partial b_{\\pi(a_j|s_t)}}\n",
    "= \\dfrac{\\partial}{\\partial b_{\\pi(a_j|s_t)}} \\left ( \\dfrac{e^f_i}{\\sum_k e^f_k} \\right )\n",
    "= \\dfrac{e^f_i \\dfrac{\\partial f_i}{\\partial b_{\\pi(a_j|s_t)}} \\left ( \\sum_k e^f_k \\right ) - e^f_i \\dfrac{\\partial}{\\partial b_{\\pi(a_j|s_t)}} \\left ( \\sum_k e^f_k \\right )}{\\left ( \\sum_k e^f_k \\right )^2} = \n",
    "\\left\\{\\begin{matrix}\n",
    "    \\dfrac{e^f_i \\sum_k e^f_k - e^{2f_i}}{\\left ( \\sum_k e^f_k \\right )^2} \n",
    "    = \\dfrac{e^f_i}{\\sum_k e^f_k} - \\left ( \\dfrac{e^f_i}{\\sum_k e^f_k} \\right )^2\n",
    "    = \\pi(a_i|s_t) - (\\pi(a_i|s_t))^2\n",
    "    = \\pi(a_i|s_t)(1 - \\pi(a_i|s_t)) \\text{ if } i = j \\\\\n",
    "    \\dfrac{(0) \\sum_k e^f_k - e^{f_i}e^{f_j}}{\\left ( \\sum_k e^f_k \\right )^2}\n",
    "    = \\left ( \\dfrac{e^f_i}{\\sum_k e^f_k} \\right ) \\left ( \\dfrac{e^f_j}{\\sum_k e^f_k} \\right )\n",
    "    = -\\pi(a_i|s_t)\\pi(a_j|s_t) \\text{ if } i \\neq j\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "Similarly, the expressions for $\\frac{\\partial \\pi(a_i|s_t)}{\\partial w_{\\pi(a_j|s_t)}^{(m)}}$ are the same as above except multiplied by $\\frac{\\partial f_j}{\\partial w_{\\pi(a_j|s_t)}^{(m)}} = x^{(m)}$:\n",
    "\n",
    "$\\dfrac{\\partial \\pi(a_i|s_t)}{\\partial b_{\\pi(a_j|s_t)}} =\n",
    "\\left\\{\\begin{matrix}\n",
    "    = x^{(m)}\\pi(a_i|s_t)(1 - \\pi(a_i|s_t)) \\text{ if } i = j \\\\\n",
    "    = -x^{(m)}\\pi(a_i|s_t)\\pi(a_j|s_t) \\text{ if } i \\neq j\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "We can plug in the numbers above to check these values:\n",
    "\n",
    "$\\frac{\\partial L_\\pi}{\\partial \\pi(a_0|s_t)} = -\\frac{-0.6363 - -0.3326}{0.4321} + (0.01)(1 + log(0.4321)) = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias is inside a softmax function!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$L_{Q} = \\sum_{i} (r_i + \\gamma Q'(s_t,a_t) - Q(s_t,a_t))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  4, 10], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_session = tf.Session()\n",
    "x = tf.constant([3, 2, 5])\n",
    "y = tf.constant([2])\n",
    "mul = x * y\n",
    "test_session.run(mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vizdoom)",
   "language": "python",
   "name": "vizdoom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
