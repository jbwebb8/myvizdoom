{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "Playing with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If using one or multiple GPUs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom RNN cells\n",
    "I'll first create my own basic RNN cells to test my understanding of how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN cell, naive input\n",
    "This is a simple RNN cell that simply applies linear transformations to the state and input at time t, adds them together, and applies the `tanh` non-linearity (as depicted in [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
    "\n",
    "![image](./rnn/SimpleRNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_cell:\n",
    "    def __init__(self, x, num_hidden, num_output, activation='tanh'):\n",
    "        # Get input shape\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        batch_size = input_shape[0]\n",
    "        input_size = input_shape[1] # assume flattened\n",
    "        \n",
    "        # Set weight matrices\n",
    "        self.W_xh = tf.Variable(tf.truncated_normal([input_size, num_hidden]), name='W_xh')\n",
    "        self.W_hh = tf.Variable(tf.truncated_normal([num_hidden, num_hidden]), name='W_hh')\n",
    "        self.W_hy = tf.Variable(tf.truncated_normal([num_hidden, num_output]), name='W_hy')\n",
    "        \n",
    "        # Define hidden state and input\n",
    "        self.state = tf.placeholder(tf.float32, shape=[batch_size, num_hidden], name='hidden_state')\n",
    "        self.zero_state = np.zeros([batch_size, num_hidden])\n",
    "        self.current_state = self.zero_state\n",
    "        \n",
    "        # Define computations\n",
    "        self.x = x\n",
    "        self.h1 = tf.matmul(self.x, self.W_xh) # input transform\n",
    "        self.h2 = tf.matmul(self.state, self.W_hh) # hidden transform\n",
    "        self.s = tf.tanh(tf.add(self.h1, self.h2)) # update state\n",
    "        self.y = tf.matmul(self.s, self.W_hy)\n",
    "        \n",
    "        # tf session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def step(self, x):\n",
    "        # Compute output and new state\n",
    "        y, new_state = self.sess.run([self.y, self.s], \n",
    "                                     feed_dict={self.state: self.current_state, self.x: x})\n",
    "        # Update current state\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def out(self, X):\n",
    "        self.current_state = self.zero_state\n",
    "        y = []\n",
    "        for x_t in X:\n",
    "            y.append(self.step(x_t))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 2\n",
    "input_size = 4\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, input_size])\n",
    "num_hidden = 3\n",
    "num_output = 1\n",
    "rnn = RNN_cell(x, num_hidden, num_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [-0.72981167]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.5832566  -0.2486003   0.43648246]]\n",
      "\n",
      "[[ 0.        ]\n",
      " [ 0.18836218]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.51001525  0.59857416  0.27309674]]\n",
      "\n",
      "[[ 0.        ]\n",
      " [ 0.30364376]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [-0.24466427  0.20748833 -0.41599485]]\n",
      "\n",
      "[[ 0.        ]\n",
      " [-1.24082625]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.59718233 -0.70386559  0.4279916 ]]\n",
      "\n",
      "[[ 0.        ]\n",
      " [ 0.13348949]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.77958661  0.69025028  0.59718585]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = [[0, 0, 0, 0], [1, 1, 1, 1]]\n",
    "    y = rnn.step(x)\n",
    "    print(y)\n",
    "    print(rnn.current_state)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [-0.72981167]]\n",
      "[[ 0.        ]\n",
      " [ 0.18836218]]\n",
      "[[ 0.        ]\n",
      " [ 0.30364376]]\n",
      "[[ 0.        ]\n",
      " [-1.24082625]]\n",
      "[[ 0.        ]\n",
      " [ 0.13348949]]\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.77958661  0.69025028  0.59718585]]\n"
     ]
    }
   ],
   "source": [
    "# Time major axis\n",
    "X = np.ones([5, batch_size, input_size])\n",
    "X[:, 0, :] = 0\n",
    "y = rnn.out(X)\n",
    "[print(y_t) for y_t in y]\n",
    "print(rnn.current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN cell, sequential input\n",
    "Rather than passing inputs from individual time steps with shape `[t, batch_size, ...]` through the RNN multiple times, we can instead pass input that has sequences stacked along the batch dimension, i.e. with shape `[t * batch_size, ...]` of the form $\\{x^1_1, ..., x^1_n, x^2_1, ..., x^2_n, x^3_1, ...\\}$ and then let the RNN cell take care of reshaping into time sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]\n",
      "  [ 1.  1.  1.]]]\n",
      "\n",
      "B[0]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "B[1]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "B[2]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Sample parameters\n",
    "batch_size = 3\n",
    "trace_length = 2\n",
    "\n",
    "# shape = [batch_size * trace_length, ...] in form shown above\n",
    "A = np.ones([batch_size * trace_length, 3, 3])\n",
    "for i in range(2):\n",
    "    A[i::2, ...] = i\n",
    "print(\"A\")\n",
    "print(A) # each batch contains a 3x3 matrix of zeros followed by 3x3 matrix of ones\n",
    "print()\n",
    "\n",
    "# reshape to [batch_size, trace_]\n",
    "B = np.reshape(A, [3, 2, 3, 3])\n",
    "for i in range(B.shape[0]):\n",
    "    print(\"B[%d]\" % i)\n",
    "    print(B[i, 0]) # should print 3x3 matrix of zeros\n",
    "    print(B[i, 1]) # should print 3x3 matrix of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_cell:\n",
    "    def __init__(self, x, num_hidden, num_output, activation='tanh'): \n",
    "        # Get input shape\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        batch_size = input_shape[0]\n",
    "        trace_length = input_shape[1]\n",
    "        input_size = input_shape[2] # assume flattened\n",
    "        self.x = tf.placeholder(tf.float32, shape=[batch_size, input_size]) # input placeholder (or input layer)\n",
    "        \n",
    "        # Set weight matrices\n",
    "        self.W_xh = tf.Variable(tf.truncated_normal([input_size, num_hidden]), name='W_xh')\n",
    "        self.W_hh = tf.Variable(tf.truncated_normal([num_hidden, num_hidden]), name='W_hh')\n",
    "        self.W_hy = tf.Variable(tf.truncated_normal([num_hidden, num_output]), name='W_hy')\n",
    "        \n",
    "        # Define hidden state and input\n",
    "        self.state = tf.placeholder(tf.float32, shape=[batch_size, num_hidden], name='hidden_state')\n",
    "        self.zero_state = np.zeros([batch_size, num_hidden])\n",
    "        self.current_state = self.zero_state\n",
    "        \n",
    "        # Set up main computational graph\n",
    "        self.h1 = tf.matmul(self.x, self.W_xh) # input transform\n",
    "        self.h2 = tf.matmul(self.state, self.W_hh) # hidden transform\n",
    "        self.s = tf.tanh(tf.add(self.h1, self.h2)) # update state\n",
    "        self.y = tf.matmul(self.s, self.W_hy) # output at time t\n",
    "        \n",
    "        # tf session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def step(self, x):\n",
    "        # shape(x) = [batch_size, input_size] at time t\n",
    "        # Compute output and new state\n",
    "        y, new_state = self.sess.run([self.y, self.s], \n",
    "                                     feed_dict={self.state: self.current_state, self.x: x})\n",
    "        # Update current state\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def out(self, X):\n",
    "        # Reset state to zero state\n",
    "        self.current_state = self.zero_state\n",
    "        \n",
    "        # Pass batch input x_t for each time step t\n",
    "        y = []\n",
    "        for i in range(X.shape[1]):\n",
    "            y.append(self.step(X[:, i, ...]))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.11198759],\n",
      "       [ 0.88573521]], dtype=float32), array([[ 1.48039114],\n",
      "       [ 1.31830537]], dtype=float32), array([[ 1.42966747],\n",
      "       [ 1.2085216 ]], dtype=float32), array([[ 1.28310418],\n",
      "       [ 1.6828239 ]], dtype=float32), array([[ 1.30481863],\n",
      "       [ 1.18696034]], dtype=float32)]\n",
      "[[ 0.29764178  0.47977298  0.99106669]\n",
      " [ 0.02432045  0.49872658  0.99757266]]\n"
     ]
    }
   ],
   "source": [
    "# Set up RNN cell\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "trace_length = 5\n",
    "input_size = 4\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, trace_length, input_size])\n",
    "num_hidden = 3\n",
    "num_output = 1\n",
    "rnn = RNN_cell(x, num_hidden, num_output)\n",
    "\n",
    "# Same test run\n",
    "x = np.random.random([batch_size, trace_length, input_size]) \n",
    "y = rnn.out(x)\n",
    "print(y)\n",
    "print(rnn.current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cell\n",
    "Now let's try to build an LSTM cell, following this graphical depiction from [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/):\n",
    "\n",
    "![image](./rnn/LSTM.png)\n",
    "\n",
    "A small note: we will be concatenating the input and hidden states rather than simply adding them to be consistent with the notation in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_cell:\n",
    "    def __init__(self, x, num_hidden, activation='tanh'): \n",
    "        # Get input shape\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        batch_size = input_shape[0]\n",
    "        trace_length = input_shape[1]\n",
    "        input_size = input_shape[2] # assume flattened\n",
    "        self.x = tf.placeholder(tf.float32, \n",
    "                                shape=[batch_size, input_size],\n",
    "                                name='x_t') # input placeholder (or input layer)\n",
    "        \n",
    "        # Zero state\n",
    "        self.zero_state = np.zeros([batch_size, num_hidden])\n",
    "\n",
    "        # Cell state\n",
    "        self.cell_state = tf.placeholder(tf.float32,\n",
    "                                         shape=[batch_size, num_hidden],\n",
    "                                         name='cell_state')\n",
    "        self.current_cell_state = self.zero_state\n",
    "\n",
    "        # Hidden state\n",
    "        self.hidden_state = tf.placeholder(tf.float32, \n",
    "                                           shape=[batch_size, num_hidden], \n",
    "                                           name='hidden_state')\n",
    "        self.current_hidden_state = self.zero_state\n",
    "\n",
    "        # Forget gate\n",
    "        with tf.name_scope(\"forget_gate\"):\n",
    "            self.W_f = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_f')\n",
    "            self.b_f = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_f\")\n",
    "            self.f_t = tf.matmul(tf.concat([self.hidden_state, self.x], 1), self.W_f) + self.b_f\n",
    "            self.f_t = tf.sigmoid(self.f_t, name='f_t')\n",
    "        \n",
    "        # Input gate\n",
    "        with tf.name_scope(\"input_gate\"):\n",
    "            self.W_i = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_i')\n",
    "            self.b_i = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_i\")\n",
    "            self.i_t = tf.matmul(tf.concat([self.hidden_state, self.x], 1), self.W_i) + self.b_i\n",
    "            self.i_t = tf.sigmoid(self.i_t, name='i_t')\n",
    "        \n",
    "        \n",
    "        # New cell state candidate values\n",
    "        with tf.name_scope(\"candidate_values\"):\n",
    "            self.W_c = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_c')\n",
    "            self.b_c = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_c\")\n",
    "            self.c_t_ = tf.matmul(tf.concat([self.hidden_state, self.x], 1), self.W_c) + self.b_c\n",
    "            self.c_t_ = tf.tanh(self.c_t_, name='c_t_')\n",
    "        \n",
    "        # Combine forget and input gates to update cell state\n",
    "        with tf.name_scope(\"update_cell_state\"):\n",
    "            self.c_t_f = tf.multiply(self.cell_state, self.f_t, name='c_t_f') \n",
    "            self.c_t_i = tf.multiply(self.c_t_, self.i_t, name='c_t_i') \n",
    "            self.c_t = tf.add(self.c_t_f, self.c_t_i, name='c_t')\n",
    "        \n",
    "        # Output gate\n",
    "        with tf.name_scope(\"output_gate\"):\n",
    "            self.W_o = tf.Variable(tf.truncated_normal([num_hidden +  input_size, num_hidden]),\n",
    "                                   name='W_o')\n",
    "            self.b_o = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name='b_o')\n",
    "            self.o_t = tf.matmul(tf.concat([self.hidden_state, self.x], 1), self.W_o) + self.b_o\n",
    "            self.o_t = tf.sigmoid(self.o_t, name='o_t')\n",
    "        \n",
    "        # Gate tanh(cell state) with output gate to update hidden state (output)\n",
    "        with tf.name_scope(\"update_hidden_state\"):\n",
    "            self.h_t = tf.multiply(tf.tanh(self.c_t), self.o_t, name='h_t')\n",
    "        \n",
    "        # tf session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def step(self, x):\n",
    "        # shape(x) = [batch_size, input_size] at time t\n",
    "        # Compute output and new state\n",
    "        h, c = self.sess.run([self.h_t, self.c_t], \n",
    "                             feed_dict={self.x: x,\n",
    "                                        self.hidden_state: self.current_hidden_state, \n",
    "                                        self.cell_state: self.current_cell_state})\n",
    "        # Update current states\n",
    "        self.current_hidden_state = h\n",
    "        self.current_cell_state = c\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def out(self, X):\n",
    "        # Reset state to zero state\n",
    "        self.current_hidden_state = self.zero_state\n",
    "        self.current_cell_state = self.zero_state\n",
    "        \n",
    "        # Pass batch input x_t for each time step t\n",
    "        H = []\n",
    "        for i in range(X.shape[1]):\n",
    "            H.append(self.step(X[:, i, ...]))\n",
    "        \n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states:\n",
      "[[ 0.36249086  0.03932692  0.03911257]\n",
      " [ 0.20759627  0.2687242  -0.01895547]]\n",
      "[[ 0.10290282 -0.02434576 -0.03873252]\n",
      " [ 0.14209694  0.2538662  -0.0651268 ]]\n",
      "[[ 0.23239931 -0.05251402  0.01512763]\n",
      " [ 0.23350267  0.22753493 -0.03943847]]\n",
      "[[ 0.17305562 -0.07541554 -0.06053226]\n",
      " [ 0.33283624  0.10467365  0.08489635]]\n",
      "[[ 0.39625847  0.02193981  0.01964775]\n",
      " [ 0.18227693  0.02154228  0.03121651]]\n",
      "\n",
      "current hidden state:\n",
      "[[ 0.39625847  0.02193981  0.01964775]\n",
      " [ 0.18227693  0.02154228  0.03121651]]\n",
      "\n",
      "current cell state:\n",
      "[[ 0.51729691  0.02303421  0.05977021]\n",
      " [ 0.26779944  0.02440291  0.07869279]]\n"
     ]
    }
   ],
   "source": [
    "# Set up RNN cell\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "trace_length = 5\n",
    "input_size = 4\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, trace_length, input_size])\n",
    "num_hidden = 3\n",
    "rnn = LSTM_cell(x, num_hidden)\n",
    "\n",
    "# Same test run\n",
    "x = np.random.random([batch_size, trace_length, input_size]) \n",
    "h = rnn.out(x)\n",
    "print(\"hidden states:\")\n",
    "for h_t in h: print(h_t)\n",
    "print(\"\\ncurrent hidden state:\")\n",
    "print(rnn.current_hidden_state)\n",
    "print(\"\\ncurrent cell state:\")\n",
    "print(rnn.current_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing tf graph\n",
    "Note: this may only work in Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit: https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter/38192374#38192374\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.8761629910688349&quot;).pbtxt = 'node {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 4\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;x_t&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 4\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cell_state&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_state&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;forget_gate/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;forget_gate/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;forget_gate/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;forget_gate/truncated_normal/mul&quot;\\n  input: &quot;forget_gate/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/W_f&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/W_f/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;forget_gate/W_f&quot;\\n  input: &quot;forget_gate/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@forget_gate/W_f&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/W_f/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;forget_gate/W_f&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@forget_gate/W_f&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/b_f&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/b_f/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;forget_gate/b_f&quot;\\n  input: &quot;forget_gate/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@forget_gate/b_f&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/b_f/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;forget_gate/b_f&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@forget_gate/b_f&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;hidden_state&quot;\\n  input: &quot;x_t&quot;\\n  input: &quot;forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;forget_gate/concat&quot;\\n  input: &quot;forget_gate/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;forget_gate/MatMul&quot;\\n  input: &quot;forget_gate/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;input_gate/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;input_gate/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;input_gate/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;input_gate/truncated_normal/mul&quot;\\n  input: &quot;input_gate/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/W_i&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/W_i/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;input_gate/W_i&quot;\\n  input: &quot;input_gate/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@input_gate/W_i&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/W_i/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;input_gate/W_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@input_gate/W_i&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/b_i&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/b_i/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;input_gate/b_i&quot;\\n  input: &quot;input_gate/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@input_gate/b_i&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/b_i/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;input_gate/b_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@input_gate/b_i&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;hidden_state&quot;\\n  input: &quot;x_t&quot;\\n  input: &quot;input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;input_gate/concat&quot;\\n  input: &quot;input_gate/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;input_gate/MatMul&quot;\\n  input: &quot;input_gate/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;candidate_values/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;candidate_values/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;candidate_values/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;candidate_values/truncated_normal/mul&quot;\\n  input: &quot;candidate_values/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/W_c&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/W_c/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;candidate_values/W_c&quot;\\n  input: &quot;candidate_values/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@candidate_values/W_c&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/W_c/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;candidate_values/W_c&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@candidate_values/W_c&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/b_c&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/b_c/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;candidate_values/b_c&quot;\\n  input: &quot;candidate_values/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@candidate_values/b_c&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/b_c/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;candidate_values/b_c&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@candidate_values/b_c&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;hidden_state&quot;\\n  input: &quot;x_t&quot;\\n  input: &quot;candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;candidate_values/concat&quot;\\n  input: &quot;candidate_values/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;candidate_values/MatMul&quot;\\n  input: &quot;candidate_values/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;cell_state&quot;\\n  input: &quot;forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;candidate_values/c_t_&quot;\\n  input: &quot;input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;update_cell_state/c_t_f&quot;\\n  input: &quot;update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;output_gate/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output_gate/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;output_gate/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output_gate/truncated_normal/mul&quot;\\n  input: &quot;output_gate/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/W_o&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/W_o/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output_gate/W_o&quot;\\n  input: &quot;output_gate/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_gate/W_o&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/W_o/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output_gate/W_o&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_gate/W_o&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/b_o&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/b_o/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output_gate/b_o&quot;\\n  input: &quot;output_gate/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_gate/b_o&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/b_o/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output_gate/b_o&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_gate/b_o&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;hidden_state&quot;\\n  input: &quot;x_t&quot;\\n  input: &quot;output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;output_gate/concat&quot;\\n  input: &quot;output_gate/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output_gate/MatMul&quot;\\n  input: &quot;output_gate/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;update_hidden_state/Tanh&quot;\\n  input: &quot;output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^forget_gate/W_f/Assign&quot;\\n  input: &quot;^forget_gate/b_f/Assign&quot;\\n  input: &quot;^input_gate/W_i/Assign&quot;\\n  input: &quot;^input_gate/b_i/Assign&quot;\\n  input: &quot;^candidate_values/W_c/Assign&quot;\\n  input: &quot;^candidate_values/b_c/Assign&quot;\\n  input: &quot;^output_gate/W_o/Assign&quot;\\n  input: &quot;^output_gate/b_o/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.8761629910688349&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unrolling the network\n",
    "The graph above looks good, but instead of iterating through a `for` loop to compute each time step, let's unroll the network to compute the output at all time steps at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_layer: \n",
    "    def __init__(self, x, num_hidden, activation='tanh'):\n",
    "        # Get input shape\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        self.batch_size = input_shape[0]\n",
    "        trace_length = input_shape[1]\n",
    "        self.input_size = input_shape[2] # assume flattened\n",
    "        self.x = x\n",
    "        x_series = tf.unstack(x, axis=1)\n",
    "        \n",
    "        # Zero state\n",
    "        self.zero_state = np.zeros([batch_size, num_hidden])\n",
    "        self.init_hidden_state = tf.placeholder(tf.float32,\n",
    "                                                shape=[batch_size, num_hidden],\n",
    "                                                name='init_hidden_state')\n",
    "        self.init_cell_state = tf.placeholder(tf.float32,\n",
    "                                              shape=[batch_size, num_hidden],\n",
    "                                              name='init_cell_state')\n",
    "        \n",
    "        # Create shared parameters\n",
    "        with tf.name_scope(\"params\"):\n",
    "            self.W_f = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_f')\n",
    "            self.b_f = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_f\")\n",
    "            self.W_i = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_i')\n",
    "            self.b_i = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_i\")\n",
    "            self.W_c = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_c')\n",
    "            self.b_c = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_c\")\n",
    "            self.W_o = tf.Variable(tf.truncated_normal([num_hidden +  input_size, num_hidden]),\n",
    "                                   name='W_o')\n",
    "            self.b_o = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name='b_o')\n",
    "        \n",
    "        # Unroll network by creating (trace_length) LSTM cells\n",
    "        h_t = self.init_hidden_state\n",
    "        c_t = self.init_cell_state\n",
    "        self.outputs = []\n",
    "        for t, x_t in enumerate(x_series):\n",
    "            h_t, c_t = self.LSTM_cell(x_t, h_t, c_t, scope=\"Cell_%d\" % t)\n",
    "            self.outputs.append(h_t)\n",
    "        \n",
    "        # tf session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def LSTM_cell(self, x_t, hidden_state, cell_state, scope=\"Cell\"):\n",
    "        with tf.name_scope(scope):\n",
    "            # Forget gate\n",
    "            with tf.name_scope(\"forget_gate\"):\n",
    "                W_f = self.W_f # vs. tf.identity(self.W_f)\n",
    "                b_f = self.b_f # vs. tf.identity(self.b_f)\n",
    "                f_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_f) + b_f\n",
    "                f_t = tf.sigmoid(f_t, name='f_t')\n",
    "\n",
    "            # Input gate\n",
    "            with tf.name_scope(\"input_gate\"):\n",
    "                W_i = self.W_i\n",
    "                b_i = self.b_i\n",
    "                i_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_i) + b_i\n",
    "                i_t = tf.sigmoid(i_t, name='i_t')\n",
    "\n",
    "\n",
    "            # New cell state candidate values\n",
    "            with tf.name_scope(\"candidate_values\"):\n",
    "                W_c = self.W_c\n",
    "                b_c = self.b_c\n",
    "                c_t_ = tf.matmul(tf.concat([hidden_state, x_t], 1), W_c) + b_c\n",
    "                c_t_ = tf.tanh(c_t_, name='c_t_')\n",
    "\n",
    "            # Combine forget and input gates to update cell state\n",
    "            with tf.name_scope(\"update_cell_state\"):\n",
    "                c_t_f = tf.multiply(cell_state, f_t, name='c_t_f') \n",
    "                c_t_i = tf.multiply(c_t_, i_t, name='c_t_i') \n",
    "                c_t = tf.add(c_t_f, c_t_i, name='c_t')\n",
    "\n",
    "            # Output gate\n",
    "            with tf.name_scope(\"output_gate\"):\n",
    "                W_o = self.W_o\n",
    "                b_o = self.b_o\n",
    "                o_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_o) + b_o\n",
    "                o_t = tf.sigmoid(o_t, name='o_t')\n",
    "\n",
    "            # Gate tanh(cell state) with output gate to update hidden state (output)\n",
    "            with tf.name_scope(\"update_hidden_state\"):\n",
    "                h_t = tf.multiply(tf.tanh(c_t), o_t, name='h_t')\n",
    "\n",
    "            return h_t, c_t\n",
    "    \n",
    "    def out(self, X):\n",
    "        feed_dict = {self.x: X,\n",
    "                     self.init_hidden_state: self.zero_state,\n",
    "                     self.init_cell_state: self.zero_state}\n",
    "        return self.sess.run(self.outputs, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states:\n",
      "[[-0.07845152 -0.0468897  -0.12943874]\n",
      " [ 0.02393308  0.01062461  0.01333745]]\n",
      "[[-0.18784374 -0.04632721 -0.15332571]\n",
      " [ 0.13834111 -0.04746186 -0.08977975]]\n",
      "[[-0.30638924 -0.14702731 -0.2577512 ]\n",
      " [ 0.10506167 -0.11854228 -0.17134102]]\n",
      "[[-0.38088018 -0.01861375 -0.27257231]\n",
      " [ 0.19053105 -0.08923865 -0.26499707]]\n",
      "[[-0.40826714  0.09289505 -0.21621533]\n",
      " [ 0.17739724 -0.09497022 -0.2219989 ]]\n"
     ]
    }
   ],
   "source": [
    "# Set up RNN cell\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "trace_length = 5\n",
    "input_size = 4\n",
    "x = tf.placeholder(tf.float32, \n",
    "                   shape=[batch_size, trace_length, input_size],\n",
    "                   name='input_series')\n",
    "num_hidden = 3\n",
    "rnn = LSTM_layer(x, num_hidden)\n",
    "\n",
    "# Same test run\n",
    "x = np.random.random([batch_size, trace_length, input_size]) \n",
    "h = rnn.out(x)\n",
    "print(\"hidden states:\")\n",
    "for h_t in h: print(h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.9457453197801575&quot;).pbtxt = 'node {\\n  name: &quot;input_series&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 4\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;unstack&quot;\\n  op: &quot;Unpack&quot;\\n  input: &quot;input_series&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n  attr {\\n    key: &quot;num&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init_hidden_state&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init_cell_state&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;params/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;params/truncated_normal/mul&quot;\\n  input: &quot;params/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_f&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_f/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/W_f&quot;\\n  input: &quot;params/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_f&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_f/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/W_f&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_f&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_f&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_f/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/b_f&quot;\\n  input: &quot;params/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_f&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_f/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/b_f&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_f&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;params/truncated_normal_1/TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_1/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;params/truncated_normal_1/mul&quot;\\n  input: &quot;params/truncated_normal_1/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_i&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_i/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/W_i&quot;\\n  input: &quot;params/truncated_normal_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_i&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_i/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/W_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_i&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_i&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_i/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/b_i&quot;\\n  input: &quot;params/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_i&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_i/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/b_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_i&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;params/truncated_normal_2/TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_2/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_2&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;params/truncated_normal_2/mul&quot;\\n  input: &quot;params/truncated_normal_2/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_c&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_c/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/W_c&quot;\\n  input: &quot;params/truncated_normal_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_c&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_c/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/W_c&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_c&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_c&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_c/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/b_c&quot;\\n  input: &quot;params/Const_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_c&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_c/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/b_c&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_c&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;params/truncated_normal_3/TruncatedNormal&quot;\\n  input: &quot;params/truncated_normal_3/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/truncated_normal_3&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;params/truncated_normal_3/mul&quot;\\n  input: &quot;params/truncated_normal_3/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_o&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_o/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/W_o&quot;\\n  input: &quot;params/truncated_normal_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_o&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/W_o/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/W_o&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/W_o&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/Const_3&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_o&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_o/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;params/b_o&quot;\\n  input: &quot;params/Const_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_o&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;params/b_o/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;params/b_o&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@params/b_o&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;init_hidden_state&quot;\\n  input: &quot;unstack&quot;\\n  input: &quot;Cell_0/forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_0/forget_gate/concat&quot;\\n  input: &quot;params/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_0/forget_gate/MatMul&quot;\\n  input: &quot;params/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_0/forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;init_hidden_state&quot;\\n  input: &quot;unstack&quot;\\n  input: &quot;Cell_0/input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_0/input_gate/concat&quot;\\n  input: &quot;params/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_0/input_gate/MatMul&quot;\\n  input: &quot;params/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_0/input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;init_hidden_state&quot;\\n  input: &quot;unstack&quot;\\n  input: &quot;Cell_0/candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_0/candidate_values/concat&quot;\\n  input: &quot;params/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_0/candidate_values/MatMul&quot;\\n  input: &quot;params/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_0/candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;init_cell_state&quot;\\n  input: &quot;Cell_0/forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_0/candidate_values/c_t_&quot;\\n  input: &quot;Cell_0/input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_0/update_cell_state/c_t_f&quot;\\n  input: &quot;Cell_0/update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;init_hidden_state&quot;\\n  input: &quot;unstack&quot;\\n  input: &quot;Cell_0/output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_0/output_gate/concat&quot;\\n  input: &quot;params/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_0/output_gate/MatMul&quot;\\n  input: &quot;params/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_0/output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_0/update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_0/update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_0/update_hidden_state/Tanh&quot;\\n  input: &quot;Cell_0/output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_0/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:1&quot;\\n  input: &quot;Cell_1/forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_1/forget_gate/concat&quot;\\n  input: &quot;params/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_1/forget_gate/MatMul&quot;\\n  input: &quot;params/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_1/forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_0/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:1&quot;\\n  input: &quot;Cell_1/input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_1/input_gate/concat&quot;\\n  input: &quot;params/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_1/input_gate/MatMul&quot;\\n  input: &quot;params/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_1/input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_0/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:1&quot;\\n  input: &quot;Cell_1/candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_1/candidate_values/concat&quot;\\n  input: &quot;params/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_1/candidate_values/MatMul&quot;\\n  input: &quot;params/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_1/candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_0/update_cell_state/c_t&quot;\\n  input: &quot;Cell_1/forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_1/candidate_values/c_t_&quot;\\n  input: &quot;Cell_1/input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_1/update_cell_state/c_t_f&quot;\\n  input: &quot;Cell_1/update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_0/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:1&quot;\\n  input: &quot;Cell_1/output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_1/output_gate/concat&quot;\\n  input: &quot;params/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_1/output_gate/MatMul&quot;\\n  input: &quot;params/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_1/output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_1/update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_1/update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_1/update_hidden_state/Tanh&quot;\\n  input: &quot;Cell_1/output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_1/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:2&quot;\\n  input: &quot;Cell_2/forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_2/forget_gate/concat&quot;\\n  input: &quot;params/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_2/forget_gate/MatMul&quot;\\n  input: &quot;params/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_2/forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_1/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:2&quot;\\n  input: &quot;Cell_2/input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_2/input_gate/concat&quot;\\n  input: &quot;params/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_2/input_gate/MatMul&quot;\\n  input: &quot;params/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_2/input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_1/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:2&quot;\\n  input: &quot;Cell_2/candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_2/candidate_values/concat&quot;\\n  input: &quot;params/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_2/candidate_values/MatMul&quot;\\n  input: &quot;params/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_2/candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_1/update_cell_state/c_t&quot;\\n  input: &quot;Cell_2/forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_2/candidate_values/c_t_&quot;\\n  input: &quot;Cell_2/input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_2/update_cell_state/c_t_f&quot;\\n  input: &quot;Cell_2/update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_1/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:2&quot;\\n  input: &quot;Cell_2/output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_2/output_gate/concat&quot;\\n  input: &quot;params/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_2/output_gate/MatMul&quot;\\n  input: &quot;params/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_2/output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_2/update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_2/update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_2/update_hidden_state/Tanh&quot;\\n  input: &quot;Cell_2/output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_2/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:3&quot;\\n  input: &quot;Cell_3/forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_3/forget_gate/concat&quot;\\n  input: &quot;params/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_3/forget_gate/MatMul&quot;\\n  input: &quot;params/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_3/forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_2/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:3&quot;\\n  input: &quot;Cell_3/input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_3/input_gate/concat&quot;\\n  input: &quot;params/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_3/input_gate/MatMul&quot;\\n  input: &quot;params/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_3/input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_2/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:3&quot;\\n  input: &quot;Cell_3/candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_3/candidate_values/concat&quot;\\n  input: &quot;params/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_3/candidate_values/MatMul&quot;\\n  input: &quot;params/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_3/candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_2/update_cell_state/c_t&quot;\\n  input: &quot;Cell_3/forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_3/candidate_values/c_t_&quot;\\n  input: &quot;Cell_3/input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_3/update_cell_state/c_t_f&quot;\\n  input: &quot;Cell_3/update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_2/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:3&quot;\\n  input: &quot;Cell_3/output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_3/output_gate/concat&quot;\\n  input: &quot;params/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_3/output_gate/MatMul&quot;\\n  input: &quot;params/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_3/output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_3/update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_3/update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_3/update_hidden_state/Tanh&quot;\\n  input: &quot;Cell_3/output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/forget_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/forget_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_3/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:4&quot;\\n  input: &quot;Cell_4/forget_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/forget_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_4/forget_gate/concat&quot;\\n  input: &quot;params/W_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/forget_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_4/forget_gate/MatMul&quot;\\n  input: &quot;params/b_f/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/forget_gate/f_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_4/forget_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/input_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/input_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_3/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:4&quot;\\n  input: &quot;Cell_4/input_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/input_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_4/input_gate/concat&quot;\\n  input: &quot;params/W_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/input_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_4/input_gate/MatMul&quot;\\n  input: &quot;params/b_i/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/input_gate/i_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_4/input_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/candidate_values/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/candidate_values/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_3/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:4&quot;\\n  input: &quot;Cell_4/candidate_values/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/candidate_values/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_4/candidate_values/concat&quot;\\n  input: &quot;params/W_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/candidate_values/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_4/candidate_values/MatMul&quot;\\n  input: &quot;params/b_c/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/candidate_values/c_t_&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_4/candidate_values/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/update_cell_state/c_t_f&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_3/update_cell_state/c_t&quot;\\n  input: &quot;Cell_4/forget_gate/f_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/update_cell_state/c_t_i&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_4/candidate_values/c_t_&quot;\\n  input: &quot;Cell_4/input_gate/i_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/update_cell_state/c_t&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_4/update_cell_state/c_t_f&quot;\\n  input: &quot;Cell_4/update_cell_state/c_t_i&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/output_gate/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/output_gate/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Cell_3/update_hidden_state/h_t&quot;\\n  input: &quot;unstack:4&quot;\\n  input: &quot;Cell_4/output_gate/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/output_gate/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Cell_4/output_gate/concat&quot;\\n  input: &quot;params/W_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/output_gate/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Cell_4/output_gate/MatMul&quot;\\n  input: &quot;params/b_o/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/output_gate/o_t&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Cell_4/output_gate/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/update_hidden_state/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Cell_4/update_cell_state/c_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cell_4/update_hidden_state/h_t&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Cell_4/update_hidden_state/Tanh&quot;\\n  input: &quot;Cell_4/output_gate/o_t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^params/W_f/Assign&quot;\\n  input: &quot;^params/b_f/Assign&quot;\\n  input: &quot;^params/W_i/Assign&quot;\\n  input: &quot;^params/b_i/Assign&quot;\\n  input: &quot;^params/W_c/Assign&quot;\\n  input: &quot;^params/b_c/Assign&quot;\\n  input: &quot;^params/W_o/Assign&quot;\\n  input: &quot;^params/b_o/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.9457453197801575&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing backprop\n",
    "The previous networks were only defined for the forward passes. In order to train them, we need to implement some form of backpropagation through time (BPTT). Rather than backpropagating entire sequences, which can be thousands to tens of thousands steps, backpropagation is often \"cut off\" after some specified length in a process termed \"truncated backpropagation\". Say you have a sequence of length $n$ with a truncated backpropagation length of $m$. Every $k_1$ timesteps, truncated backprogation performs BPTT for $k_2$ timesteps. There are two basic approaches:\n",
    "\n",
    "1) Set $k_1=k_2$. That is, if BPTT is truncated to 10 timesteps backward, then BPTT is only performed every 10 timesteps. In other words, for a sequence $\\{x_1, \\ldots, x_n\\}$, the network learns from sequences $\\{x_1, \\ldots, x_m\\}, \\{x_{m+1}, \\ldots, x_{2m}\\}, \\ldots, \\{x_{n-m}, \\ldots, x_n\\}$. This is the approach that TensorFlow uses.\n",
    "\n",
    "2) Set $k_1=1$. That is, perform BPTT every timestep, regardless of BPTT length. In other words, for a sequence $\\{x_1, \\ldots, x_n\\}$, the network learns from sequences $\\{x_1, \\ldots, x_m\\}, \\{x_2, \\ldots, x_{m+1}\\}, \\ldots, \\{x_{n-m}, \\ldots, x_n\\}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the class to build the graph, not perform computations\n",
    "class LSTM_layer: \n",
    "    def __init__(self, x, num_hidden, activation='tanh'):\n",
    "        # Get input shape\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        self.batch_size = input_shape[0]\n",
    "        trace_length = input_shape[1]\n",
    "        self.input_size = input_shape[2] # assume flattened\n",
    "        x_series = tf.unstack(x, axis=1)\n",
    "        \n",
    "        # Initial states\n",
    "        self.init_hidden_state = tf.placeholder(tf.float32,\n",
    "                                                shape=[batch_size, num_hidden],\n",
    "                                                name='init_hidden_state')\n",
    "        self.init_cell_state = tf.placeholder(tf.float32,\n",
    "                                              shape=[batch_size, num_hidden],\n",
    "                                              name='init_cell_state')\n",
    "        \n",
    "        # Create shared parameters\n",
    "        with tf.name_scope(\"params\"):\n",
    "            self.W_f = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_f')\n",
    "            self.b_f = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_f\")\n",
    "            self.W_i = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_i')\n",
    "            self.b_i = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_i\")\n",
    "            self.W_c = tf.Variable(tf.truncated_normal([num_hidden + input_size, num_hidden]),\n",
    "                                   name='W_c')\n",
    "            self.b_c = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name=\"b_c\")\n",
    "            self.W_o = tf.Variable(tf.truncated_normal([num_hidden +  input_size, num_hidden]),\n",
    "                                   name='W_o')\n",
    "            self.b_o = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_hidden]),\n",
    "                                   name='b_o')\n",
    "        \n",
    "        # Unroll network by creating (trace_length) LSTM cells\n",
    "        h_t = self.init_hidden_state\n",
    "        c_t = self.init_cell_state\n",
    "        self.hidden_states = []\n",
    "        self.cell_states = []\n",
    "        for t, x_t in enumerate(x_series):\n",
    "            h_t, c_t = self.LSTM_cell(x_t, h_t, c_t, scope=\"Cell_%d\" % t)\n",
    "            self.hidden_states.append(h_t)\n",
    "            self.cell_states.append(c_t)\n",
    "        \n",
    "    def LSTM_cell(self, x_t, hidden_state, cell_state, scope=\"Cell\"):\n",
    "        with tf.name_scope(scope):\n",
    "            # Forget gate\n",
    "            with tf.name_scope(\"forget_gate\"):\n",
    "                W_f = self.W_f # vs. tf.identity(self.W_f)\n",
    "                b_f = self.b_f # vs. tf.identity(self.b_f)\n",
    "                f_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_f) + b_f\n",
    "                f_t = tf.sigmoid(f_t, name='f_t')\n",
    "\n",
    "            # Input gate\n",
    "            with tf.name_scope(\"input_gate\"):\n",
    "                W_i = self.W_i\n",
    "                b_i = self.b_i\n",
    "                i_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_i) + b_i\n",
    "                i_t = tf.sigmoid(i_t, name='i_t')\n",
    "\n",
    "\n",
    "            # New cell state candidate values\n",
    "            with tf.name_scope(\"candidate_values\"):\n",
    "                W_c = self.W_c\n",
    "                b_c = self.b_c\n",
    "                c_t_ = tf.matmul(tf.concat([hidden_state, x_t], 1), W_c) + b_c\n",
    "                c_t_ = tf.tanh(c_t_, name='c_t_')\n",
    "\n",
    "            # Combine forget and input gates to update cell state\n",
    "            with tf.name_scope(\"update_cell_state\"):\n",
    "                c_t_f = tf.multiply(cell_state, f_t, name='c_t_f') \n",
    "                c_t_i = tf.multiply(c_t_, i_t, name='c_t_i') \n",
    "                c_t = tf.add(c_t_f, c_t_i, name='c_t')\n",
    "\n",
    "            # Output gate\n",
    "            with tf.name_scope(\"output_gate\"):\n",
    "                W_o = self.W_o\n",
    "                b_o = self.b_o\n",
    "                o_t = tf.matmul(tf.concat([hidden_state, x_t], 1), W_o) + b_o\n",
    "                o_t = tf.sigmoid(o_t, name='o_t')\n",
    "\n",
    "            # Gate tanh(cell state) with output gate to update hidden state (output)\n",
    "            with tf.name_scope(\"update_hidden_state\"):\n",
    "                h_t = tf.multiply(tf.tanh(c_t), o_t, name='h_t')\n",
    "\n",
    "            return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:\n",
      "[[ 0.54066342  0.45933658]]\n",
      "[[ 0.53943419  0.46056589]]\n",
      "[[ 0.54027724  0.4597227 ]]\n",
      "labels:\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "losses:\n",
      "0.777972\n",
      "0.775299\n",
      "0.615673\n"
     ]
    }
   ],
   "source": [
    "# Set up input\n",
    "tf.reset_default_graph()\n",
    "batch_size = 1\n",
    "trace_length = 3\n",
    "input_size = 4\n",
    "x = tf.placeholder(tf.float32, \n",
    "                   shape=[batch_size, trace_length, input_size],\n",
    "                   name='input_series')\n",
    "\n",
    "# Add RNN cell\n",
    "num_hidden = 3\n",
    "rnn = LSTM_layer(x, num_hidden)\n",
    "\n",
    "# Add simple softmax output layer\n",
    "preds = []\n",
    "num_output = 2\n",
    "W_p = tf.Variable(tf.truncated_normal([num_hidden, num_output]), name='W_p')\n",
    "b_p = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_output]), name='b_p')\n",
    "with tf.name_scope(\"preds\"):\n",
    "    for h_t in rnn.hidden_states:\n",
    "        preds.append(tf.nn.softmax(tf.matmul(h_t, W_p) + b_p))\n",
    "\n",
    "# Add loss function\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, trace_length, num_output], \n",
    "                         name='y')\n",
    "y_series = tf.unstack(y, axis=1)\n",
    "losses = []\n",
    "with tf.name_scope(\"losses\"):\n",
    "    for i, [pred_t, y_t] in enumerate(zip(preds, y_series)):\n",
    "        with tf.name_scope(\"loss_%d\" % i):\n",
    "            losses.append(tf.reduce_sum(-y_t * tf.log(pred_t)))\n",
    "    total_loss = tf.reduce_mean(losses, name='total_loss')\n",
    "\n",
    "# Add optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(0.1)\n",
    "train_step = optimizer.minimize(total_loss)\n",
    "    \n",
    "# Grab gradients for interest\n",
    "    \n",
    "    \n",
    "# Add session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Sample backpropagation step\n",
    "x_batch = np.random.random([batch_size, trace_length, input_size])\n",
    "y_batch = np.random.random([batch_size, trace_length, num_output])\n",
    "y_batch[:, :, 1] = 1 - y_batch[:, :, 0]\n",
    "y_batch = (y_batch > 0.5).astype(int) # creates one-hot vectors\n",
    "zero_state = np.zeros([batch_size, num_hidden])\n",
    "feed_dict = {x: x_batch,\n",
    "             rnn.init_hidden_state: zero_state,\n",
    "             rnn.init_cell_state: zero_state,\n",
    "             y: y_batch}\n",
    "preds_batch, losses_batch, _ = sess.run([preds, losses, train_step], \n",
    "                                        feed_dict=feed_dict)\n",
    "print(\"preds:\")\n",
    "for p in preds_batch: print(p)\n",
    "\n",
    "print(\"labels:\")\n",
    "for y_t in y_batch: print(y_t)\n",
    "    \n",
    "print(\"losses:\")\n",
    "for l in losses_batch: print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the gradients that TensorFlow is calculating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try when you dare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it on some toy data set. Let's use the Echo-RNN set from [this blog post](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY, truncated_backprop_length):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.45\n",
      "Epoch 2 loss: 3.46\n",
      "Epoch 3 loss: 3.32\n",
      "Epoch 4 loss: 3.18\n",
      "Epoch 5 loss: 2.83\n",
      "Epoch 6 loss: 2.73\n",
      "Epoch 7 loss: 2.87\n",
      "Epoch 8 loss: 2.82\n",
      "Epoch 9 loss: 2.78\n",
      "Epoch 10 loss: 2.46\n",
      "Epoch 11 loss: 2.17\n",
      "Epoch 12 loss: 2.78\n",
      "Epoch 13 loss: 2.34\n",
      "Epoch 14 loss: 2.21\n",
      "Epoch 15 loss: 2.78\n",
      "Epoch 16 loss: 1.68\n",
      "Epoch 17 loss: 1.74\n",
      "Epoch 18 loss: 2.27\n",
      "Epoch 19 loss: 1.49\n",
      "Epoch 20 loss: 1.97\n",
      "Epoch 21 loss: 1.61\n",
      "Epoch 22 loss: 1.55\n",
      "Epoch 23 loss: 1.78\n",
      "Epoch 24 loss: 2.04\n",
      "Epoch 25 loss: 1.23\n",
      "Epoch 26 loss: 1.28\n",
      "Epoch 27 loss: 1.68\n",
      "Epoch 28 loss: 1.28\n",
      "Epoch 29 loss: 1.17\n",
      "Epoch 30 loss: 1.53\n",
      "Epoch 31 loss: 1.97\n",
      "Epoch 32 loss: 2.34\n",
      "Epoch 33 loss: 1.00\n",
      "Epoch 34 loss: 1.07\n",
      "Epoch 35 loss: 1.06\n",
      "Epoch 36 loss: 1.86\n",
      "Epoch 37 loss: 1.50\n",
      "Epoch 38 loss: 1.08\n",
      "Epoch 39 loss: 1.58\n",
      "Epoch 40 loss: 1.27\n",
      "Epoch 41 loss: 0.93\n",
      "Epoch 42 loss: 1.00\n",
      "Epoch 43 loss: 1.32\n",
      "Epoch 44 loss: 1.16\n",
      "Epoch 45 loss: 1.05\n",
      "Epoch 46 loss: 1.88\n",
      "Epoch 47 loss: 1.12\n",
      "Epoch 48 loss: 1.26\n",
      "Epoch 49 loss: 2.25\n",
      "Epoch 50 loss: 1.28\n",
      "Epoch 51 loss: 1.30\n",
      "Epoch 52 loss: 1.52\n",
      "Epoch 53 loss: 1.22\n",
      "Epoch 54 loss: 1.00\n",
      "Epoch 55 loss: 1.19\n",
      "Epoch 56 loss: 0.78\n",
      "Epoch 57 loss: 1.31\n",
      "Epoch 58 loss: 0.79\n",
      "Epoch 59 loss: 0.92\n",
      "Epoch 60 loss: 0.79\n",
      "Epoch 61 loss: 0.57\n",
      "Epoch 62 loss: 0.49\n",
      "Epoch 63 loss: 0.80\n",
      "Epoch 64 loss: 0.41\n",
      "Epoch 65 loss: 0.32\n",
      "Epoch 66 loss: 0.31\n",
      "Epoch 67 loss: 0.25\n",
      "Epoch 68 loss: 0.38\n",
      "Epoch 69 loss: 0.21\n",
      "Epoch 70 loss: 0.22\n",
      "Epoch 71 loss: 0.25\n",
      "Epoch 72 loss: 0.21\n",
      "Epoch 73 loss: 0.16\n",
      "Epoch 74 loss: 0.18\n",
      "Epoch 75 loss: 0.17\n",
      "Epoch 76 loss: 0.13\n",
      "Epoch 77 loss: 0.22\n",
      "Epoch 78 loss: 0.15\n",
      "Epoch 79 loss: 0.15\n",
      "Epoch 80 loss: 0.18\n",
      "Epoch 81 loss: 0.19\n",
      "Epoch 82 loss: 0.13\n",
      "Epoch 83 loss: 0.33\n",
      "Epoch 84 loss: 0.16\n",
      "Epoch 85 loss: 0.16\n",
      "Epoch 86 loss: 0.11\n",
      "Epoch 87 loss: 0.13\n",
      "Epoch 88 loss: 0.10\n",
      "Epoch 89 loss: 0.11\n",
      "Epoch 90 loss: 0.10\n",
      "Epoch 91 loss: 0.10\n",
      "Epoch 92 loss: 0.10\n",
      "Epoch 93 loss: 0.08\n",
      "Epoch 94 loss: 0.08\n",
      "Epoch 95 loss: 0.11\n",
      "Epoch 96 loss: 0.09\n",
      "Epoch 97 loss: 0.15\n",
      "Epoch 98 loss: 0.08\n",
      "Epoch 99 loss: 0.07\n",
      "Epoch 100 loss: 0.07\n",
      "Epoch 101 loss: 0.09\n",
      "Epoch 102 loss: 0.07\n",
      "Epoch 103 loss: 0.08\n",
      "Epoch 104 loss: 0.08\n",
      "Epoch 105 loss: 0.08\n",
      "Epoch 106 loss: 0.09\n",
      "Epoch 107 loss: 0.08\n",
      "Epoch 108 loss: 0.08\n",
      "Epoch 109 loss: 0.09\n",
      "Epoch 110 loss: 0.07\n",
      "Epoch 111 loss: 0.08\n",
      "Epoch 112 loss: 0.08\n",
      "Epoch 113 loss: 0.08\n",
      "Epoch 114 loss: 0.08\n",
      "Epoch 115 loss: 0.08\n",
      "Epoch 116 loss: 0.08\n",
      "Epoch 117 loss: 0.07\n",
      "Epoch 118 loss: 0.07\n",
      "Epoch 119 loss: 0.06\n",
      "Epoch 120 loss: 0.06\n",
      "Epoch 121 loss: 0.07\n",
      "Epoch 122 loss: 0.06\n",
      "Epoch 123 loss: 0.04\n",
      "Epoch 124 loss: 0.05\n",
      "Epoch 125 loss: 0.06\n",
      "Epoch 126 loss: 0.05\n",
      "Epoch 127 loss: 0.07\n",
      "Epoch 128 loss: 0.05\n",
      "Epoch 129 loss: 0.43\n",
      "Epoch 130 loss: 0.07\n",
      "Epoch 131 loss: 0.05\n",
      "Epoch 132 loss: 0.05\n",
      "Epoch 133 loss: 0.05\n",
      "Epoch 134 loss: 0.06\n",
      "Epoch 135 loss: 0.05\n",
      "Epoch 136 loss: 0.05\n",
      "Epoch 137 loss: 0.06\n",
      "Epoch 138 loss: 0.05\n",
      "Epoch 139 loss: 0.05\n",
      "Epoch 140 loss: 0.05\n",
      "Epoch 141 loss: 0.06\n",
      "Epoch 142 loss: 0.05\n",
      "Epoch 143 loss: 0.06\n",
      "Epoch 144 loss: 0.05\n",
      "Epoch 145 loss: 0.05\n",
      "Epoch 146 loss: 0.06\n",
      "Epoch 147 loss: 0.06\n",
      "Epoch 148 loss: 0.06\n",
      "Epoch 149 loss: 0.05\n",
      "Epoch 150 loss: 0.05\n",
      "Epoch 151 loss: 0.04\n",
      "Epoch 152 loss: 0.04\n",
      "Epoch 153 loss: 0.04\n",
      "Epoch 154 loss: 0.04\n",
      "Epoch 155 loss: 0.04\n",
      "Epoch 156 loss: 0.03\n",
      "Epoch 157 loss: 0.04\n",
      "Epoch 158 loss: 0.04\n",
      "Epoch 159 loss: 0.03\n",
      "Epoch 160 loss: 0.03\n",
      "Epoch 161 loss: 0.03\n",
      "Epoch 162 loss: 0.04\n",
      "Epoch 163 loss: 0.03\n",
      "Epoch 164 loss: 0.04\n",
      "Epoch 165 loss: 0.04\n",
      "Epoch 166 loss: 0.04\n",
      "Epoch 167 loss: 0.03\n",
      "Epoch 168 loss: 0.04\n",
      "Epoch 169 loss: 0.04\n",
      "Epoch 170 loss: 0.05\n",
      "Epoch 171 loss: 0.03\n",
      "Epoch 172 loss: 0.03\n",
      "Epoch 173 loss: 0.03\n",
      "Epoch 174 loss: 0.03\n",
      "Epoch 175 loss: 0.03\n",
      "Epoch 176 loss: 0.04\n",
      "Epoch 177 loss: 0.04\n",
      "Epoch 178 loss: 0.04\n",
      "Epoch 179 loss: 0.03\n",
      "Epoch 180 loss: 0.03\n",
      "Epoch 181 loss: 0.03\n",
      "Epoch 182 loss: 0.03\n",
      "Epoch 183 loss: 0.03\n",
      "Epoch 184 loss: 0.03\n",
      "Epoch 185 loss: 0.03\n",
      "Epoch 186 loss: 0.04\n",
      "Epoch 187 loss: 0.03\n",
      "Epoch 188 loss: 0.03\n",
      "Epoch 189 loss: 0.03\n",
      "Epoch 190 loss: 0.04\n",
      "Epoch 191 loss: 0.03\n",
      "Epoch 192 loss: 0.02\n",
      "Epoch 193 loss: 0.04\n",
      "Epoch 194 loss: 0.06\n",
      "Epoch 195 loss: 0.06\n",
      "Epoch 196 loss: 0.04\n",
      "Epoch 197 loss: 0.03\n",
      "Epoch 198 loss: 0.03\n",
      "Epoch 199 loss: 0.03\n",
      "Epoch 200 loss: 0.03\n",
      "Epoch 201 loss: 0.02\n",
      "Epoch 202 loss: 0.03\n",
      "Epoch 203 loss: 0.03\n",
      "Epoch 204 loss: 0.04\n",
      "Epoch 205 loss: 0.03\n",
      "Epoch 206 loss: 0.03\n",
      "Epoch 207 loss: 0.03\n",
      "Epoch 208 loss: 0.03\n",
      "Epoch 209 loss: 0.03\n",
      "Epoch 210 loss: 0.03\n",
      "Epoch 211 loss: 0.03\n",
      "Epoch 212 loss: 0.02\n",
      "Epoch 213 loss: 0.02\n",
      "Epoch 214 loss: 0.02\n",
      "Epoch 215 loss: 0.02\n",
      "Epoch 216 loss: 0.03\n",
      "Epoch 217 loss: 0.02\n",
      "Epoch 218 loss: 0.02\n",
      "Epoch 219 loss: 0.03\n",
      "Epoch 220 loss: 0.03\n",
      "Epoch 221 loss: 0.03\n",
      "Epoch 222 loss: 0.03\n",
      "Epoch 223 loss: 0.02\n",
      "Epoch 224 loss: 0.03\n",
      "Epoch 225 loss: 0.03\n",
      "Epoch 226 loss: 0.02\n",
      "Epoch 227 loss: 0.02\n",
      "Epoch 228 loss: 0.02\n",
      "Epoch 229 loss: 0.02\n",
      "Epoch 230 loss: 0.02\n",
      "Epoch 231 loss: 0.03\n",
      "Epoch 232 loss: 0.02\n",
      "Epoch 233 loss: 0.02\n",
      "Epoch 234 loss: 0.02\n",
      "Epoch 235 loss: 0.03\n",
      "Epoch 236 loss: 0.02\n",
      "Epoch 237 loss: 0.02\n",
      "Epoch 238 loss: 0.02\n",
      "Epoch 239 loss: 0.02\n",
      "Epoch 240 loss: 0.03\n",
      "Epoch 241 loss: 0.02\n",
      "Epoch 242 loss: 0.02\n",
      "Epoch 243 loss: 0.02\n",
      "Epoch 244 loss: 0.02\n",
      "Epoch 245 loss: 0.03\n",
      "Epoch 246 loss: 0.02\n",
      "Epoch 247 loss: 0.02\n",
      "Epoch 248 loss: 0.03\n",
      "Epoch 249 loss: 0.02\n",
      "Epoch 250 loss: 0.04\n",
      "Epoch 251 loss: 0.02\n",
      "Epoch 252 loss: 0.03\n",
      "Epoch 253 loss: 0.03\n",
      "Epoch 254 loss: 0.02\n",
      "Epoch 255 loss: 0.02\n",
      "Epoch 256 loss: 0.02\n",
      "Epoch 257 loss: 0.03\n",
      "Epoch 258 loss: 0.02\n",
      "Epoch 259 loss: 0.02\n",
      "Epoch 260 loss: 0.03\n",
      "Epoch 261 loss: 0.02\n",
      "Epoch 262 loss: 0.02\n",
      "Epoch 263 loss: 0.02\n",
      "Epoch 264 loss: 0.02\n",
      "Epoch 265 loss: 0.06\n",
      "Epoch 266 loss: 0.05\n",
      "Epoch 267 loss: 0.03\n",
      "Epoch 268 loss: 0.03\n",
      "Epoch 269 loss: 0.03\n",
      "Epoch 270 loss: 0.04\n",
      "Epoch 271 loss: 0.03\n",
      "Epoch 272 loss: 0.02\n",
      "Epoch 273 loss: 0.03\n",
      "Epoch 274 loss: 0.02\n",
      "Epoch 275 loss: 0.02\n",
      "Epoch 276 loss: 0.02\n",
      "Epoch 277 loss: 0.02\n",
      "Epoch 278 loss: 0.02\n",
      "Epoch 279 loss: 0.02\n",
      "Epoch 280 loss: 0.04\n",
      "Epoch 281 loss: 0.02\n",
      "Epoch 282 loss: 0.02\n",
      "Epoch 283 loss: 0.02\n",
      "Epoch 284 loss: 0.02\n",
      "Epoch 285 loss: 0.02\n",
      "Epoch 286 loss: 0.02\n",
      "Epoch 287 loss: 0.02\n",
      "Epoch 288 loss: 0.02\n",
      "Epoch 289 loss: 0.02\n",
      "Epoch 290 loss: 0.02\n",
      "Epoch 291 loss: 0.02\n",
      "Epoch 292 loss: 0.02\n",
      "Epoch 293 loss: 0.02\n",
      "Epoch 294 loss: 0.02\n",
      "Epoch 295 loss: 0.02\n",
      "Epoch 296 loss: 0.01\n",
      "Epoch 297 loss: 0.02\n",
      "Epoch 298 loss: 0.02\n",
      "Epoch 299 loss: 0.02\n",
      "Epoch 300 loss: 0.02\n",
      "Epoch 301 loss: 0.02\n",
      "Epoch 302 loss: 0.02\n",
      "Epoch 303 loss: 0.02\n",
      "Epoch 304 loss: 0.02\n",
      "Epoch 305 loss: 0.02\n",
      "Epoch 306 loss: 0.02\n",
      "Epoch 307 loss: 0.02\n",
      "Epoch 308 loss: 0.02\n",
      "Epoch 309 loss: 0.02\n",
      "Epoch 310 loss: 0.02\n",
      "Epoch 311 loss: 0.02\n",
      "Epoch 312 loss: 0.02\n",
      "Epoch 313 loss: 0.02\n",
      "Epoch 314 loss: 0.02\n",
      "Epoch 315 loss: 0.01\n",
      "Epoch 316 loss: 0.02\n",
      "Epoch 317 loss: 0.02\n",
      "Epoch 318 loss: 0.02\n",
      "Epoch 319 loss: 0.01\n",
      "Epoch 320 loss: 0.02\n",
      "Epoch 321 loss: 0.02\n",
      "Epoch 322 loss: 0.02\n",
      "Epoch 323 loss: 0.02\n",
      "Epoch 324 loss: 0.01\n",
      "Epoch 325 loss: 0.01\n",
      "Epoch 326 loss: 0.01\n",
      "Epoch 327 loss: 0.01\n",
      "Epoch 328 loss: 0.01\n",
      "Epoch 329 loss: 0.01\n",
      "Epoch 330 loss: 0.01\n",
      "Epoch 331 loss: 0.01\n",
      "Epoch 332 loss: 0.02\n",
      "Epoch 333 loss: 0.01\n",
      "Epoch 334 loss: 0.01\n",
      "Epoch 335 loss: 0.02\n",
      "Epoch 336 loss: 0.02\n",
      "Epoch 337 loss: 0.01\n",
      "Epoch 338 loss: 0.02\n",
      "Epoch 339 loss: 0.02\n",
      "Epoch 340 loss: 0.01\n",
      "Epoch 341 loss: 0.01\n",
      "Epoch 342 loss: 0.02\n",
      "Epoch 343 loss: 0.01\n",
      "Epoch 344 loss: 0.02\n",
      "Epoch 345 loss: 0.02\n",
      "Epoch 346 loss: 0.02\n",
      "Epoch 347 loss: 0.02\n",
      "Epoch 348 loss: 0.01\n",
      "Epoch 349 loss: 0.02\n",
      "Epoch 350 loss: 0.02\n",
      "Epoch 351 loss: 0.02\n",
      "Epoch 352 loss: 0.01\n",
      "Epoch 353 loss: 0.01\n",
      "Epoch 354 loss: 0.02\n",
      "Epoch 355 loss: 0.02\n",
      "Epoch 356 loss: 0.02\n",
      "Epoch 357 loss: 0.01\n",
      "Epoch 358 loss: 0.02\n",
      "Epoch 359 loss: 0.02\n",
      "Epoch 360 loss: 0.02\n",
      "Epoch 361 loss: 0.02\n",
      "Epoch 362 loss: 0.01\n",
      "Epoch 363 loss: 0.01\n",
      "Epoch 364 loss: 0.01\n",
      "Epoch 365 loss: 0.02\n",
      "Epoch 366 loss: 0.01\n",
      "Epoch 367 loss: 0.01\n",
      "Epoch 368 loss: 0.02\n",
      "Epoch 369 loss: 0.01\n",
      "Epoch 370 loss: 0.01\n",
      "Epoch 371 loss: 0.02\n",
      "Epoch 372 loss: 0.01\n",
      "Epoch 373 loss: 0.02\n",
      "Epoch 374 loss: 0.02\n",
      "Epoch 375 loss: 0.01\n",
      "Epoch 376 loss: 0.02\n",
      "Epoch 377 loss: 0.02\n",
      "Epoch 378 loss: 0.02\n",
      "Epoch 379 loss: 0.01\n",
      "Epoch 380 loss: 0.01\n",
      "Epoch 381 loss: 0.02\n",
      "Epoch 382 loss: 0.02\n",
      "Epoch 383 loss: 0.01\n",
      "Epoch 384 loss: 0.02\n",
      "Epoch 385 loss: 0.02\n",
      "Epoch 386 loss: 0.01\n",
      "Epoch 387 loss: 0.02\n",
      "Epoch 388 loss: 0.01\n",
      "Epoch 389 loss: 0.01\n",
      "Epoch 390 loss: 0.02\n",
      "Epoch 391 loss: 0.02\n",
      "Epoch 392 loss: 0.01\n",
      "Epoch 393 loss: 0.01\n",
      "Epoch 394 loss: 0.02\n",
      "Epoch 395 loss: 0.01\n",
      "Epoch 396 loss: 0.01\n",
      "Epoch 397 loss: 0.01\n",
      "Epoch 398 loss: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399 loss: 0.01\n",
      "Epoch 400 loss: 0.01\n",
      "Epoch 401 loss: 0.01\n",
      "Epoch 402 loss: 0.01\n",
      "Epoch 403 loss: 0.01\n",
      "Epoch 404 loss: 0.01\n",
      "Epoch 405 loss: 0.02\n",
      "Epoch 406 loss: 0.01\n",
      "Epoch 407 loss: 0.02\n",
      "Epoch 408 loss: 0.01\n",
      "Epoch 409 loss: 0.01\n",
      "Epoch 410 loss: 0.01\n",
      "Epoch 411 loss: 0.01\n",
      "Epoch 412 loss: 0.01\n",
      "Epoch 413 loss: 0.01\n",
      "Epoch 414 loss: 0.01\n",
      "Epoch 415 loss: 0.02\n",
      "Epoch 416 loss: 0.02\n",
      "Epoch 417 loss: 0.02\n",
      "Epoch 418 loss: 0.01\n",
      "Epoch 419 loss: 0.01\n",
      "Epoch 420 loss: 0.02\n",
      "Epoch 421 loss: 0.02\n",
      "Epoch 422 loss: 0.01\n",
      "Epoch 423 loss: 0.01\n",
      "Epoch 424 loss: 0.01\n",
      "Epoch 425 loss: 0.03\n",
      "Epoch 426 loss: 0.01\n",
      "Epoch 427 loss: 0.01\n",
      "Epoch 428 loss: 0.01\n",
      "Epoch 429 loss: 0.01\n",
      "Epoch 430 loss: 0.01\n",
      "Epoch 431 loss: 0.01\n",
      "Epoch 432 loss: 0.01\n",
      "Epoch 433 loss: 0.01\n",
      "Epoch 434 loss: 0.02\n",
      "Epoch 435 loss: 0.01\n",
      "Epoch 436 loss: 0.01\n",
      "Epoch 437 loss: 0.02\n",
      "Epoch 438 loss: 0.02\n",
      "Epoch 439 loss: 0.01\n",
      "Epoch 440 loss: 0.01\n",
      "Epoch 441 loss: 0.02\n",
      "Epoch 442 loss: 0.02\n",
      "Epoch 443 loss: 0.01\n",
      "Epoch 444 loss: 0.01\n",
      "Epoch 445 loss: 0.01\n",
      "Epoch 446 loss: 0.01\n",
      "Epoch 447 loss: 0.01\n",
      "Epoch 448 loss: 0.01\n",
      "Epoch 449 loss: 0.01\n",
      "Epoch 450 loss: 0.01\n",
      "Epoch 451 loss: 0.01\n",
      "Epoch 452 loss: 0.01\n",
      "Epoch 453 loss: 0.01\n",
      "Epoch 454 loss: 0.01\n",
      "Epoch 455 loss: 0.02\n",
      "Epoch 456 loss: 0.01\n",
      "Epoch 457 loss: 0.01\n",
      "Epoch 458 loss: 0.01\n",
      "Epoch 459 loss: 0.02\n",
      "Epoch 460 loss: 0.01\n",
      "Epoch 461 loss: 0.01\n",
      "Epoch 462 loss: 0.01\n",
      "Epoch 463 loss: 0.01\n",
      "Epoch 464 loss: 0.01\n",
      "Epoch 465 loss: 0.02\n",
      "Epoch 466 loss: 0.01\n",
      "Epoch 467 loss: 0.02\n",
      "Epoch 468 loss: 0.02\n",
      "Epoch 469 loss: 0.01\n",
      "Epoch 470 loss: 0.01\n",
      "Epoch 471 loss: 0.02\n",
      "Epoch 472 loss: 0.01\n",
      "Epoch 473 loss: 0.01\n",
      "Epoch 474 loss: 0.01\n",
      "Epoch 475 loss: 0.02\n",
      "Epoch 476 loss: 0.01\n",
      "Epoch 477 loss: 0.01\n",
      "Epoch 478 loss: 0.01\n",
      "Epoch 479 loss: 0.01\n",
      "Epoch 480 loss: 0.02\n",
      "Epoch 481 loss: 0.01\n",
      "Epoch 482 loss: 0.01\n",
      "Epoch 483 loss: 0.01\n",
      "Epoch 484 loss: 0.01\n",
      "Epoch 485 loss: 0.01\n",
      "Epoch 486 loss: 0.01\n",
      "Epoch 487 loss: 0.01\n",
      "Epoch 488 loss: 0.01\n",
      "Epoch 489 loss: 0.02\n",
      "Epoch 490 loss: 0.01\n",
      "Epoch 491 loss: 0.01\n",
      "Epoch 492 loss: 0.01\n",
      "Epoch 493 loss: 0.01\n",
      "Epoch 494 loss: 0.01\n",
      "Epoch 495 loss: 0.01\n",
      "Epoch 496 loss: 0.01\n",
      "Epoch 497 loss: 0.01\n",
      "Epoch 498 loss: 0.01\n",
      "Epoch 499 loss: 0.01\n",
      "Epoch 500 loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 500\n",
    "total_series_length = 50000\n",
    "trace_length = 15\n",
    "input_size = 1\n",
    "num_hidden = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "\n",
    "# Set up input\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, \n",
    "                   shape=[batch_size, trace_length, input_size],\n",
    "                   name='input_series')\n",
    "\n",
    "# Add RNN cell\n",
    "num_hidden = 3\n",
    "rnn = LSTM_layer(x, num_hidden)\n",
    "\n",
    "# Add simple softmax output layer\n",
    "preds = []\n",
    "W_p = tf.Variable(tf.truncated_normal([num_hidden, num_classes]), name='W_p')\n",
    "b_p = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[1, num_classes]), name='b_p')\n",
    "with tf.name_scope(\"preds\"):\n",
    "    for h_t in rnn.hidden_states:\n",
    "        preds.append(tf.nn.softmax(tf.matmul(h_t, W_p) + b_p))\n",
    "\n",
    "# Add loss function\n",
    "y = tf.placeholder(tf.int32, shape=[batch_size, trace_length], \n",
    "                         name='y')\n",
    "labels = tf.one_hot(y, 2)\n",
    "labels_series = tf.unstack(labels, axis=1)\n",
    "losses = []\n",
    "with tf.name_scope(\"losses\"):\n",
    "    for i, [pred_t, y_t] in enumerate(zip(preds, labels_series)):\n",
    "        with tf.name_scope(\"loss_%d\" % i):\n",
    "            losses.append(tf.reduce_sum(-y_t * tf.log(pred_t)))\n",
    "    total_loss = tf.reduce_mean(losses, name='total_loss')\n",
    "\n",
    "# Add optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(0.3)\n",
    "train_step = optimizer.minimize(total_loss)\n",
    "    \n",
    "# Grab gradients for interest\n",
    "    \n",
    "    \n",
    "# Add session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "# Training\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate data\n",
    "    den = batch_size * trace_length # total data length must be multiple\n",
    "    x_length = total_series_length // den * den\n",
    "    x_batch = np.array(np.random.choice(2, x_length, p=[0.5, 0.5]))\n",
    "    y_batch = np.roll(x_batch, echo_step)\n",
    "    y_batch[0:echo_step] = 0\n",
    "    x_batch = np.reshape(x_batch, [batch_size, -1, trace_length, 1])\n",
    "    y_batch = np.reshape(y_batch, [batch_size, -1, trace_length])\n",
    "    num_batches = x_batch.shape[0]\n",
    "    \n",
    "    # Set initial RNN states\n",
    "    current_hidden_state = np.zeros([batch_size, num_hidden])\n",
    "    current_cell_state = np.zeros([batch_size, num_hidden])\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # Train step\n",
    "        feed_dict = {x: x_batch[:, batch_idx],\n",
    "                     rnn.init_hidden_state: current_hidden_state,\n",
    "                     rnn.init_cell_state: current_cell_state,\n",
    "                     y: y_batch[:, batch_idx]}\n",
    "        current_hidden_state, current_cell_state, total_loss_, _ = \\\n",
    "            sess.run([rnn.hidden_states[-1], rnn.cell_states[-1], total_loss, train_step], \n",
    "                                  feed_dict=feed_dict)\n",
    "        loss_list.append(total_loss_)\n",
    "        \n",
    "    # Print progress\n",
    "    print(\"Epoch %d loss: %.2f\" % (epoch+1, total_loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nknJq9CIiKff/AURP6np23gSHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7df9aa89e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feed_dict = {x: x_batch[:, -2],\n",
    "             rnn.init_hidden_state: init_hidden_state,\n",
    "             rnn.init_cell_state: init_cell_state,\n",
    "             y: y_batch[:, -2]}\n",
    "preds_ = sess.run(preds, feed_dict=feed_dict)\n",
    "plot(loss_list, preds_, x_batch[:, -2], y_batch[:, -2], trace_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping for DRQN agent\n",
    "Since the DRQN agent will be riding the DQN n-step and trajectory replay functions, we need to be sure that we are reshaping sample memories in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 12 11 23 42]\n",
      "[38 40 42 12 14 16 11 13 15 23 25 27 42 44 46]\n",
      "[[38 40 42]\n",
      " [12 14 16]\n",
      " [11 13 15]\n",
      " [23 25 27]\n",
      " [42 44 46]]\n"
     ]
    }
   ],
   "source": [
    "total_size = 50\n",
    "sample_size = 5\n",
    "tr_len = 3\n",
    "n_step = 2\n",
    "idx = np.random.randint(0, total_size, sample_size)\n",
    "print(idx)\n",
    "x, y = np.meshgrid(idx, np.arange(tr_len) * n_step) # non-overlapping n-step sequences\n",
    "idx = np.transpose(x + y).flatten() # [i, i+1, ..., i+n, j, j+1, ..., j+n, k...]\n",
    "print(idx)\n",
    "print(idx.reshape(sample_size, tr_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing DRQN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If using one or multiple GPUs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "from network.DRQNetwork import DRQNetwork\n",
    "from helper import create_network, create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "net_file = \"../networks/drqn.json\"\n",
    "phi = 1\n",
    "channels = 1\n",
    "num_actions = 4\n",
    "alpha = 0.01\n",
    "params_file = None\n",
    "output_dir = \"../notebooks/rnn/test/\" \n",
    "sess = tf.Session()\n",
    "train_mode = True\n",
    "\n",
    "net = create_network(net_file,\n",
    "                     phi=phi, \n",
    "                     num_channels=channels, \n",
    "                     num_outputs=num_actions,\n",
    "                     learning_rate=alpha,\n",
    "                     params_file=params_file,\n",
    "                     output_directory=output_dir,\n",
    "                     session=sess,\n",
    "                     train_mode=train_mode,\n",
    "                     scope=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of agent states --> network states:\n",
      "screen --> Tensor(\"main_network/screen:0\", shape=(?, 84, 84, 3), dtype=float32)\n",
      "[GameVariable.HEALTH] --> Tensor(\"main_network/health:0\", shape=(?, 1), dtype=float32)\n",
      "[GameVariable.VELOCITY_X, GameVariable.VELOCITY_Y] --> Tensor(\"main_network/velocity:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "agent_file_path = \"../notebooks/rnn/drqn.json\"\n",
    "config_file_path = \"../config/open_field.cfg\"\n",
    "params_file_path = None\n",
    "action_set = \"basic_three\"\n",
    "results_dir = \"../notebooks/rnn/test/\"\n",
    "\n",
    "game = DoomGame()\n",
    "game.load_config(config_file_path)\n",
    "game.init()\n",
    "\n",
    "agent = create_agent(agent_file_path,\n",
    "                     game=game, \n",
    "                     params_file=params_file_path,\n",
    "                     action_set=action_set,\n",
    "                     output_directory=results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.initialize_new_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    agent.perform_learning_step(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (10, 2) (10,) (320, 84, 84, 3)\n",
      "(320, 3) (10,)\n",
      "[[-0.10492384  0.39012218 -0.34917536]\n",
      " [-0.21287759  0.31767994 -0.44692278]\n",
      " [-0.1867802   0.35419792 -0.4402622 ]\n",
      " [-0.14460289  0.39855003 -0.42988038]\n",
      " [-0.23564385  0.29714859 -0.45321113]\n",
      " [-0.1461082   0.34052545 -0.36988449]\n",
      " [-0.23667099  0.29579222 -0.44848144]\n",
      " [-0.22927527  0.2997233  -0.45408291]\n",
      " [-0.23118626  0.29701585 -0.4542383 ]\n",
      " [-0.22660463  0.29857078 -0.45613205]]\n",
      "[-0.10492384 -0.21287759 -0.4402622  -0.42988038 -0.23564385  0.34052545\n",
      " -0.23667099 -0.22927527 -0.4542383  -0.45613205]\n"
     ]
    }
   ],
   "source": [
    "s1, a, s2, isterminal, r, w, idx = agent.memory.get_sample(agent.batch_size)\n",
    "target_q = agent._get_target_q(s1, a, s2, isterminal, r)\n",
    "s1 = agent.network._check_state(s1)\n",
    "a = agent.network._check_actions(a)\n",
    "a = a[::32]\n",
    "target_q = target_q[::32]\n",
    "w = w[::32]\n",
    "feed_dict = {s_: s for s_, s in zip(agent.network.state, s1)} \n",
    "feed_dict.update({agent.network.actions: a, \n",
    "                  agent.network.target_q: target_q, \n",
    "                  agent.network.IS_weights: w})\n",
    "feed_dict = agent.network._check_train_mode(feed_dict)\n",
    "print(w.shape, a.shape, target_q.shape, s1[0].shape)\n",
    "#lstm = tf.get_default_graph().get_tensor_by_name(\"main_network/LSTM_1/rnn/transpose:0\")\n",
    "q_sa = tf.get_default_graph().get_tensor_by_name(\"main_network/loss/q_sa:0\")\n",
    "out, out2 = agent.network.sess.run([agent.network.q, q_sa], feed_dict=feed_dict)\n",
    "print(out.shape, out2.shape)\n",
    "print(out[::32, :])\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was confused why feeding in different lengths for target Q and state did not result in an error. Turns out that the tf op `gather_nd` automatically handles this case by selecting every $n^{th}$ example to make the shapes match along this dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss masking\n",
    "Routine testing for masking the first $n$ losses during RNN traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = tf.placeholder(tf.int32, shape=[], name=\"batch_size\")\n",
    "w = tf.placeholder(tf.float32, shape=[None], name=\"IS_weights\")\n",
    "mask_len = -1\n",
    "tot_len = tf.shape(w)[0]\n",
    "tr_len = tot_len // batch_size\n",
    "mask_len = tf.minimum(mask_len, tr_len)\n",
    "mask_zeros = tf.zeros([batch_size, mask_len])\n",
    "mask_ones = tf.ones([batch_size, tr_len - mask_len])\n",
    "w_ = tf.reshape(tf.concat([mask_zeros, mask_ones], axis=1), [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Dimension -1 must be >= 0\n\t [[Node: zeros = Fill[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros/shape, zeros/Const)]]\n\nCaused by op 'zeros', defined at:\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-0d88ba3453bd>\", line 9, in <module>\n    mask_zeros = tf.zeros([batch_size, mask_len])\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1360, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1166, in fill\n    result = _op_def_lib.apply_op(\"Fill\", dims=dims, value=value, name=name)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Dimension -1 must be >= 0\n\t [[Node: zeros = Fill[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros/shape, zeros/Const)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension -1 must be >= 0\n\t [[Node: zeros = Fill[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros/shape, zeros/Const)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-42c0a53dfbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m feed_dict = {batch_size: 4,\n\u001b[1;32m      3\u001b[0m              w: np.random.rand(20)}\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension -1 must be >= 0\n\t [[Node: zeros = Fill[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros/shape, zeros/Const)]]\n\nCaused by op 'zeros', defined at:\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-0d88ba3453bd>\", line 9, in <module>\n    mask_zeros = tf.zeros([batch_size, mask_len])\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1360, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1166, in fill\n    result = _op_def_lib.apply_op(\"Fill\", dims=dims, value=value, name=name)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/james/anaconda3/envs/vizdoom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Dimension -1 must be >= 0\n\t [[Node: zeros = Fill[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros/shape, zeros/Const)]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "feed_dict = {batch_size: 4,\n",
    "             w: np.random.rand(20)}\n",
    "print(sess.run(w_, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Updating RNN state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of agent states --> network states:\n",
      "screen --> Tensor(\"main_network/screen:0\", shape=(?, 84, 84, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/DRL/myvizdoom/python/agent/Agent.py:170: UserWarning: The following game variables were not used: \n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "agent_file_path = \"../notebooks/rnn/drqn.json\"\n",
    "config_file_path = \"../config/open_field.cfg\"\n",
    "params_file_path = None\n",
    "action_set = \"basic_three\"\n",
    "results_dir = \"../notebooks/rnn/test/\"\n",
    "\n",
    "game = DoomGame()\n",
    "game.load_config(config_file_path)\n",
    "game.init()\n",
    "\n",
    "agent = create_agent(agent_file_path,\n",
    "                     game=game, \n",
    "                     params_file=params_file_path,\n",
    "                     action_set=action_set,\n",
    "                     output_directory=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue has now been fixed, so the output displayed below demonstrates the correct cell update. Previously, the last two cells would have been the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[-0.10721724 -0.01644403 -0.13550684  0.01726984 -0.03495932  0.0373818\n",
      " -0.11352646  0.02683512  0.0235149  -0.04382068]\n",
      "[-0.17859283 -0.0242555  -0.2544525   0.0274361  -0.07349882  0.06319889\n",
      " -0.21599317  0.04630607  0.03474696 -0.06407228]\n"
     ]
    }
   ],
   "source": [
    "agent.network.reset_rnn_state()\n",
    "s1 = [np.ones([1] + agent.network.input_shape), np.zeros(agent.num_game_var)]\n",
    "\n",
    "for _ in range(3):\n",
    "    for s in agent.network.rnn_current_states[0][0]:\n",
    "        print(s[:10])\n",
    "    agent.network.update_rnn_state(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[-0.10721724 -0.01644403 -0.13550684  0.01726984 -0.03495932  0.0373818\n",
      " -0.11352646  0.02683512  0.0235149  -0.04382068]\n",
      "[-0.17859283 -0.0242555  -0.2544525   0.0274361  -0.07349882  0.06319889\n",
      " -0.21599317  0.04630607  0.03474696 -0.06407228]\n"
     ]
    }
   ],
   "source": [
    "agent.network.reset_rnn_state()\n",
    "s1 = [np.ones([1] + agent.network.input_shape), np.zeros(agent.num_game_var)]\n",
    "\n",
    "for _ in range(3):\n",
    "    for s in agent.network.rnn_current_states[0][0]:\n",
    "        print(s[:10])\n",
    "    \n",
    "    # Add extra line to update_rnn_state(s1)\n",
    "    s1_ = agent.network._check_state(s1)\n",
    "    feed_dict = {s_: s for s_, s in zip(agent.network.state, s1_)}\n",
    "    feed_dict.update({agent.network.batch_size: 1})\n",
    "    feed_dict.update({s_: s for s_, s in zip(agent.network.rnn_init_states, agent.network.rnn_current_states)}) # extra line\n",
    "    agent.network.rnn_current_states = agent.network.sess.run(agent.network.rnn_states, \n",
    "                                                    feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10721724 -0.01644403 -0.13550684  0.01726984 -0.03495932  0.0373818\n",
      " -0.11352646  0.02683512  0.0235149  -0.04382068]\n"
     ]
    }
   ],
   "source": [
    "s1_ = agent.network._check_state(s1)\n",
    "feed_dict = {s_: s for s_, s in zip(agent.network.state, s1_)}\n",
    "feed_dict.update({agent.network.batch_size: 1})\n",
    "#feed_dict.update({s_: s for s_, s in zip(agent.network.rnn_init_states, agent.network.rnn_current_states)}) # extra line\n",
    "agent.network.rnn_current_states = agent.network.sess.run(agent.network.rnn_states, \n",
    "                                                    feed_dict=feed_dict)\n",
    "for s in agent.network.rnn_current_states[0][0]:\n",
    "    print(s[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure that the `get_rnn_zero_states` is not being mutated by `rnn_current_states` due to passing by reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current states:\n",
      "[-0.10721724 -0.01644403 -0.13550684  0.01726984 -0.03495932  0.0373818\n",
      " -0.11352646  0.02683512  0.0235149  -0.04382068]\n",
      "zero states:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"current states:\")\n",
    "for s in agent.network.rnn_current_states[0][0]:\n",
    "    print(s[:10])\n",
    "\n",
    "print(\"zero states:\")\n",
    "for s in agent.network.get_rnn_zero_state()[0][0]:\n",
    "    print(s[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTMStateTuple(c=<tf.Tensor 'main_network/LSTM_1/LSTMCellZeroState/zeros:0' shape=(?, 128) dtype=float32>, h=<tf.Tensor 'main_network/LSTM_1/LSTMCellZeroState/zeros_1:0' shape=(?, 128) dtype=float32>)]\n"
     ]
    }
   ],
   "source": [
    "print(agent.network.rnn_init_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[-0.10171492 -0.01094173 -0.13184489  0.01610954 -0.03992078  0.03366337\n",
      " -0.111006    0.01599175  0.0190571  -0.03910401]\n",
      "[-0.16979404 -0.01485812 -0.24806482  0.02496561 -0.08228803  0.05632716\n",
      " -0.21108341  0.02814458  0.0284702  -0.05713952]\n"
     ]
    }
   ],
   "source": [
    "agent.network.reset_rnn_state()\n",
    "s1 = [np.zeros([1] + agent.network.input_shape), np.zeros(agent.num_game_var)]\n",
    "\n",
    "for _ in range(3):\n",
    "    for s in agent.network.rnn_current_states[0][0]:\n",
    "        print(s[:10])\n",
    "    \n",
    "    # Add extra line to update_rnn_state(s1)\n",
    "    s1_ = agent.network._check_state(s1)\n",
    "    feed_dict = {s_: s for s_, s in zip(agent.network.state, s1_)}\n",
    "    feed_dict.update({agent.network.batch_size: 1})\n",
    "    feed_dict.update({s_: s for s_, s in zip(agent.network.rnn_init_states, agent.network.rnn_current_states)}) # extra line\n",
    "    agent.network.rnn_current_states = agent.network.sess.run(agent.network.rnn_states, \n",
    "                                                    feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vizdoom)",
   "language": "python",
   "name": "vizdoom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
