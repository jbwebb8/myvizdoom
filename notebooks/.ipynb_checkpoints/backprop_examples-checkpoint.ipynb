{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Examples\n",
    "While troubleshooting some exploding gradients, I decided to walk through backpropagation for the first step, line by line. Here are a couple of examples for two different loss functions: squared error and Huber loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared error is given by:\n",
    "    \n",
    "    L = np.sum((target_Q - Q)^2)\n",
    "   \n",
    "And thus the gradient with respect to Q becomes:\n",
    "\n",
    "    dL/dQ = -2 * (target_Q - Q)\n",
    "\n",
    "Because Q is given by:\n",
    "\n",
    "    Q_i = b_i + w_i,1 * x_i,1 + w_i,2 * x_i,2 + ... + w_i,n * x_i,n\n",
    "\n",
    "where `x_i` represents the input from the previous (fully-connected) layer, the gradients with respect the biases and weights of Q are:\n",
    "\n",
    "    dL/dQ_b_i = dL/dQ * 1 = dL/dQ = -2 * (target_Q - Q)\n",
    "    dL/dQ_w_i = dL/dQ * x_i = -2 * (target_Q - Q) * x_i\n",
    "\n",
    "Let's run through an example. After initializing the variables of a DQN and copying them to holder network, we can calculate q(s, a) and target_q(s, a) via our primary and target networks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q\n",
    "[[ 5.34987879 -2.93507671 -1.3103404   1.01032877]]\n",
    "\n",
    "target_Q\n",
    "[[ 4.39664125 -2.93507671 -1.3103404   1.01032877]]\n",
    "\n",
    "loss\n",
    "0.908662\n",
    "\n",
    "sum_grad\n",
    "[[ 1.  1.  1.  1.]]\n",
    "\n",
    "square_grad\n",
    "[[-1.90647507  0.          0.          0.        ]]\n",
    "\n",
    "sub_grad\n",
    "[[ 1.90647507  0.          0.          0.        ]]\n",
    "\n",
    "Q_grad\n",
    "[(array([ 1.90647507,  0.        ,  0.        ,  0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the parameters are initially equal, the target_Q will be the same for all actions, given the initial state, except that which was chosen, in this case at index 0. The update for the target_Q is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q2 = np.max(target_network.get_q_values(s2))\n",
    "target_q = target_network.get_q_values(s1)\n",
    "target_q[a] = r + gamma * (1 - isterminal) * q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By summing over the squared differences between Q and target_Q for each element, we arrive at the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size 5\n",
    "Q:\n",
    " [[-0.53266817 -0.0878141   2.45806932  3.23200798]\n",
    " [-0.8812443   1.07983756  0.65692121  1.23270094]\n",
    " [ 0.48478067  0.3618294   1.32550728  2.08765173]\n",
    " [ 0.65240127  0.50296509  2.48937154  1.34428322]\n",
    " [-0.55477738  0.12505805  2.41784     2.32180619]]\n",
    "a:\n",
    " [[0 2]\n",
    " [1 1]\n",
    " [2 3]\n",
    " [3 3]\n",
    " [4 1]]\n",
    "target_Q:\n",
    " [-0.79592603 -4.67121124 -1.58115745  0.19765809  1.02122998]\n",
    "loss:\n",
    " [ 2.75399542  5.25104904  3.16880918  0.64662516  0.40156206]\n",
    "grads:\n",
    " [ 0.          0.10382807  1.          2.        ]\n",
    "\n",
    "# Obtained by placing this code in Network.learn between feed_dict and train_step\n",
    "q, loss_ = self.sess.run([self.q, self.loss], feed_dict=feed_dict)\n",
    "print(\"Q:\\n\", q)\n",
    "print(\"a:\\n\", a)\n",
    "print(\"target_Q:\\n\", target_q)\n",
    "print(\"loss:\\n\", loss_)\n",
    "var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                             scope=self.scope)\n",
    "optimizer = self.graph_dict[\"optimizer\"][0]\n",
    "gvs = self.sess.run(optimizer.compute_gradients(self.loss, var_list=var_list),\n",
    "                    feed_dict=feed_dict)\n",
    "print(\"grads:\\n\", gvs[-1][0])\n",
    "input(\"Press enter...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vizdoom)",
   "language": "python",
   "name": "vizdoom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
